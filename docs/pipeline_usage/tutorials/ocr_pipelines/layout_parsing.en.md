---
comments: true
---

# General Layout Parsing Pipeline Tutorial

## 1. Introduction to the General Layout Parsing Pipeline
Layout parsing is a technology that extracts structured information from document images, primarily used to convert complex document layouts into machine-readable data formats. This technology has extensive applications in document management, information extraction, and data digitization. By combining Optical Character Recognition (OCR), image processing, and machine learning algorithms, layout parsing can identify and extract text blocks, titles, paragraphs, images, tables, and other layout elements from documents. The process typically involves three main steps: layout analysis, element analysis, and data formatting, ultimately generating structured document data to improve data processing efficiency and accuracy.

The <b>General Layout Parsing Pipeline</b> includes modules for table structure recognition, layout region analysis, text detection, text recognition, formula recognition, seal text detection, text image rectification, and document image orientation classification.

<b>If you prioritize model accuracy, choose a model with higher accuracy. If you prioritize inference speed, choose a model with faster inference. If you prioritize model storage size, choose a model with a smaller storage size.</b>
<details><summary> ğŸ‘‰Model List Details</summary>
<p><b>Table Structure Recognition Module Models</b>:</p>
<table>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>Accuracy (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Size (M)</th>
<th>Description</th>
</tr>
<tr>
<td>SLANet</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/SLANet_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/SLANet_pretrained.pdparams">Training Model</a></td>
<td>59.52</td>
<td>103.08 / 103.08</td>
<td>197.99 / 197.99</td>
<td>6.9 M</td>
<td>SLANet is a table structure recognition model developed by Baidu PaddleX Team. The model significantly improves the accuracy and inference speed of table structure recognition by adopting a CPU-friendly lightweight backbone network PP-LCNet, a high-low-level feature fusion module CSP-PAN, and a feature decoding module SLA Head that aligns structural and positional information.</td>
</tr>
<tr>
<td>SLANet_plus</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/SLANet_plus_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/SLANet_plus_pretrained.pdparams">Training Model</a></td>
<td>63.69</td>
<td>140.29 / 140.29</td>
<td>195.39 / 195.39</td>
<td>6.9 M</td>
<td>SLANet_plus is an enhanced version of SLANet, the table structure recognition model developed by Baidu PaddleX Team. Compared to SLANet, SLANet_plus significantly improves the recognition ability for wireless and complex tables and reduces the model's sensitivity to the accuracy of table positioning, enabling more accurate recognition even with offset table positioning.</td>
</tr>
</table>

<p><b>Layout Detection Module Models</b>:</p>
<table>
<thead>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>mAP(0.5) (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Storage Size (M)</th>
<th>Introduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>PP-DocLayout-L</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-DocLayout-L_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-DocLayout-L_pretrained.pdparams">Training Model</a></td>
<td>90.4</td>
<td>34.6244 / 10.3945</td>
<td>510.57 / -</td>
<td>123.76 M</td>
<td>A high-precision layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using RT-DETR-L.</td>
</tr>
<tr>
<td>PP-DocLayout-M</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-DocLayout-M_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-DocLayout-M_pretrained.pdparams">Training Model</a></td>
<td>75.2</td>
<td>13.3259 / 4.8685</td>
<td>44.0680 / 44.0680</td>
<td>22.578</td>
<td>A layout area localization model with balanced precision and efficiency, trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-L.</td>
</tr>
<tr>
<td>PP-DocLayout-S</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-DocLayout-S_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-DocLayout-S_pretrained.pdparams">Training Model</a></td>
<td>70.9</td>
<td>8.3008 / 2.3794</td>
<td>10.0623 / 9.9296</td>
<td>4.834</td>
<td>A high-efficiency layout area localization model trained on a self-built dataset containing Chinese and English papers, magazines, contracts, books, exams, and research reports using PicoDet-S.</td>
</tr>
</tbody>
</table>
<b>Note: The evaluation dataset for the above precision metrics is a self-built layout area detection dataset by PaddleOCR, containing 500 common document-type images of Chinese and English papers, magazines, contracts, books, exams, and research reports. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</b>

> â— The above list includes the <b>3 core models</b> that are key supported by the text recognition module. The module actually supports a total of <b>11 full models</b>, including several predefined models with different categories. The complete model list is as follows:

* <b>Table Layout Detection Model</b>
<table>
<thead>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>mAP(0.5) (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Storage Size (M)</th>
<th>Introduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>PicoDet_layout_1x_table</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PicoDet_layout_1x_table_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PicoDet_layout_1x_table_pretrained.pdparams">Training Model</a></td>
<td>97.5</td>
<td>8.02 / 3.09</td>
<td>23.70 / 20.41</td>
<td>7.4 M</td>
<td>A high-efficiency layout area localization model trained on a self-built dataset using PicoDet-1x, capable of detecting table regions.</td>
</tr>
</tbody></table>
<b>Note: The evaluation dataset for the above precision metrics is a self-built layout table area detection dataset by PaddleOCR, containing 7835 Chinese and English document images with tables. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</b>

* <b>3-Class Layout Detection Model, including Table, Image, and Stamp</b>
<table>
<thead>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>mAP(0.5) (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Storage Size (M)</th>
<th>Introduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>PicoDet-S_layout_3cls</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PicoDet-S_layout_3cls_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PicoDet-S_layout_3cls_pretrained.pdparams">Training Model</a></td>
<td>88.2</td>
<td>8.99 / 2.22</td>
<td>16.11 / 8.73</td>
<td>4.8</td>
<td>A high-efficiency layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-S.</td>
</tr>
<tr>
<td>PicoDet-L_layout_3cls</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PicoDet-L_layout_3cls_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PicoDet-L_layout_3cls_pretrained.pdparams">Training Model</a></td>
<td>89.0</td>
<td>13.05 / 4.50</td>
<td>41.30 / 41.30</td>
<td>22.6</td>
<td>A balanced efficiency and precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-L.</td>
</tr>
<tr>
<td>RT-DETR-H_layout_3cls</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/RT-DETR-H_layout_3cls_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/RT-DETR-H_layout_3cls_pretrained.pdparams">Training Model</a></td>
<td>95.8</td>
<td>114.93 / 27.71</td>
<td>947.56 / 947.56</td>
<td>470.1</td>
<td>A high-precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using RT-DETR-H.</td>
</tr>
</tbody></table>
<b>Note: The evaluation dataset for the above precision metrics is a self-built layout area detection dataset by PaddleOCR, containing 1154 common document images of Chinese and English papers, magazines, and research reports. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</b>

* <b>5-Class English Document Area Detection Model, including Text, Title, Table, Image, and List</b>
<table>
<thead>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>mAP(0.5) (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Storage Size (M)</th>
<th>Introduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>PicoDet_layout_1x</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PicoDet_layout_1x_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PicoDet_layout_1x_pretrained.pdparams">Training Model</a></td>
<td>97.8</td>
<td>9.03 / 3.10</td>
<td>25.82 / 20.70</td>
<td>7.4</td>
<td>A high-efficiency English document layout area localization model trained on the PubLayNet dataset using PicoDet-1x.</td>
</tr>
</tbody></table>
<b>Note: The evaluation dataset for the above precision metrics is the [PubLayNet](https://developer.ibm.com/exchanges/data/all/publaynet/) dataset, containing 11245 English document images. GPU inference time is based on an NVIDIA Tesla T4 machine with FP32 precision. CPU inference speed is based on an Intel(R) Xeon(R) Gold 5117 CPU @ 2.00GHz with 8 threads and FP32 precision.</b>

* <b>17-Class Area Detection Model, including 17 common layout categories: Paragraph Title, Image, Text, Number, Abstract, Content, Figure Caption, Formula, Table, Table Caption, References, Document Title, Footnote, Header, Algorithm, Footer, and Stamp</b>
<table>
<thead>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>mAP(0.5) (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Storage Size (M)</th>
<th>Introduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>PicoDet-S_layout_17cls</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PicoDet-S_layout_17cls_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PicoDet-S_layout_17cls_pretrained.pdparams">Training Model</a></td>
<td>87.4</td>
<td>9.11 / 2.12</td>
<td>15.42 / 9.12</td>
<td>4.8</td>
<td>A high-efficiency layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-S.</td>
</tr>
<tr>
<td>PicoDet-L_layout_17cls</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PicoDet-L_layout_17cls_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PicoDet-L_layout_17cls_pretrained.pdparams">Training Model</a></td>
<td>89.0</td>
<td>13.50 / 4.69</td>
<td>43.32 / 43.32</td>
<td>22.6</td>
<td>A balanced efficiency and precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using PicoDet-L.</td>
</tr>
<tr>
<td>RT-DETR-H_layout_17cls</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/RT-DETR-H_layout_17cls_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/RT-DETR-H_layout_17cls_pretrained.pdparams">Training Model</a></td>
<td>98.3</td>
<td>115.29 / 104.09</td>
<td>995.27 / 995.27</td>
<td>470.2</td>
<td>A high-precision layout area localization model trained on a self-built dataset of Chinese and English papers, magazines, and research reports using RT-DETR-H.</td>
</tr>
</tbody>
</table>

<p><b>Text Detection Module Models</b>:</p>
<table>
<thead>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>Detection Hmean (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Size (M)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PP-OCRv4_server_det</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-OCRv4_server_det_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv4_server_det_pretrained.pdparams">Training Model</a></td>
<td>82.69</td>
<td>83.34 / 80.91</td>
<td>442.58 / 442.58</td>
<td>109</td>
<td>PP-OCRv4's server-side text detection model, featuring higher accuracy, suitable for deployment on high-performance servers</td>
</tr>
<tr>
<td>PP-OCRv4_mobile_det</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-OCRv4_mobile_det_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv4_mobile_det_pretrained.pdparams">Training Model</a></td>
<td>77.79</td>
<td>8.79 / 3.13</td>
<td>51.00 / 28.58</td>
<td>4.7</td>
<td>PP-OCRv4's mobile text detection model, optimized for efficiency, suitable for deployment on edge devices</td>
</tr>
</tbody>
</table>

<p><b>Text Recognition Module Models</b>:</p>
* <b>Chinese Recognition Model</b>
<table>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>Recognition Avg Accuracy (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Storage Size (M)</th>
<th>Introduction</th>
</tr>
<tr>
<td>PP-OCRv4_server_rec_doc</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-OCRv4_server_rec_doc_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>81.53</td>
<td>6.65 / 2.38</td>
<td>32.92 / 32.92</td>
<td>74.7 M</td>
<td>PP-OCRv4_server_rec_doc is trained on a mixed dataset of more Chinese document data and PP-OCR training data based on PP-OCRv4_server_rec. It has added the recognition capabilities for some traditional Chinese characters, Japanese, and special characters. The number of recognizable characters is over 15,000. In addition to the improvement in document-related text recognition, it also enhances the general text recognition capability.</td>
</tr>
<tr>
<td>PP-OCRv4_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-OCRv4_mobile_rec_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv4_mobile_rec_pretrained.pdparams">Training Model</a></td>
<td>78.74</td>
<td>4.82 / 1.20</td>
<td>16.74 / 4.64</td>
<td>10.6 M</td>
<td>The lightweight recognition model of PP-OCRv4 has high inference efficiency and can be deployed on various hardware devices, including edge devices.</td>
</tr>
<tr>
<td>PP-OCRv4_server_rec </td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-OCRv4_server_rec_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv4_server_rec_pretrained.pdparams">Training Model</a></td>
<td>80.61 </td>
<td>6.58 / 2.43</td>
<td>33.17 / 33.17</td>
<td>71.2 M</td>
<td>The server-side model of PP-OCRv4 offers high inference accuracy and can be deployed on various types of servers.</td>
</tr>
<tr>
<td>PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>72.96</td>
<td>5.87 / 1.19</td>
<td>9.07 / 4.28</td>
<td>9.2 M</td>
<td>PP-OCRv3â€™s lightweight recognition model is designed for high inference efficiency and can be deployed on a variety of hardware devices, including edge devices.</td>
</tr>
</table>

<table>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>Recognition Avg Accuracy (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Size (M)</th>
<th>Description</th>
</tr>
<tr>
<td>ch_SVTRv2_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/ch_SVTRv2_rec_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/ch_SVTRv2_rec_pretrained.pdparams">Training Model</a></td>
<td>68.81</td>
<td>8.08 / 2.74</td>
<td>50.17 / 42.50</td>
<td>73.9 M</td>
<td rowspan="1">
SVTRv2 is a server text recognition model developed by the OpenOCR team of Fudan University's Visual and Learning Laboratory (FVL). It won the first prize in the PaddleOCR Algorithm Model Challenge - Task One: OCR End-to-End Recognition Task. The end-to-end recognition accuracy on the A list is 6% higher than that of PP-OCRv4.
</td>
</tr>
</table>

<table>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>Recognition Avg Accuracy (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Size (M)</th>
<th>Description</th>
</tr>
<tr>
<td>ch_RepSVTR_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/ch_RepSVTR_rec_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/ch_RepSVTR_rec_pretrained.pdparams">Training Model</a></td>
<td>65.07</td>
<td>5.93 / 1.62</td>
<td>20.73 / 7.32</td>
<td>22.1 M</td>
<td rowspan="1">    The RepSVTR text recognition model is a mobile text recognition model based on SVTRv2. It won the first prize in the PaddleOCR Algorithm Model Challenge - Task One: OCR End-to-End Recognition Task. The end-to-end recognition accuracy on the B list is 2.5% higher than that of PP-OCRv4, with the same inference speed.</td>
</tr>
</table>

* <b>English Recognition Model</b>
<table>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>Recognition Avg Accuracy(%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Storage Size (M)</th>
<th>Introduction</th>
</tr>
<tr>
<td>en_PP-OCRv4_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/en_PP-OCRv4_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td> 70.39</td>
<td>4.81 / 0.75</td>
<td>16.10 / 5.31</td>
<td>6.8 M</td>
<td>The ultra-lightweight English recognition model trained based on the PP-OCRv4 recognition model supports the recognition of English and numbers.</td>
</tr>
<tr>
<td>en_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/en_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>70.69</td>
<td>5.44 / 0.75</td>
<td>8.65 / 5.57</td>
<td>7.8 M </td>
<td>The ultra-lightweight English recognition model trained based on the PP-OCRv3 recognition model supports the recognition of English and numbers.</td>
</tr>
</table>

* <b>Multilingual Recognition Model</b>
<table>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>Recognition Avg Accuracy(%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Storage Size (M)</th>
<th>Introduction</th>
</tr>
<tr>
<td>korean_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/korean_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>60.21</td>
<td>5.40 / 0.97</td>
<td>9.11 / 4.05</td>
<td>8.6 M</td>
<td>The ultra-lightweight Korean recognition model trained based on the PP-OCRv3 recognition model supports the recognition of Korean and numbers. </td>
</tr>
<tr>
<td>japan_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/japan_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>45.69</td>
<td>5.70 / 1.02</td>
<td>8.48 / 4.07</td>
<td>8.8 M </td>
<td>The ultra-lightweight Japanese recognition model trained based on the PP-OCRv3 recognition model supports the recognition of Japanese and numbers.</td>
</tr>
<tr>
<td>chinese_cht_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/chinese_cht_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>82.06</td>
<td>5.90 / 1.28</td>
<td>9.28 / 4.34</td>
<td>9.7 M </td>
<td>The ultra-lightweight Traditional Chinese recognition model trained based on the PP-OCRv3 recognition model supports the recognition of Traditional Chinese and numbers.</td>
</tr>
<tr>
<td>te_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/te_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>95.88</td>
<td>5.42 / 0.82</td>
<td>8.10 / 6.91</td>
<td>7.8 M </td>
<td>The ultra-lightweight Telugu recognition model trained based on the PP-OCRv3 recognition model supports the recognition of Telugu and numbers.</td>
</tr>
<tr>
<td>ka_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/ka_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>96.96</td>
<td>5.25 / 0.79</td>
<td>9.09 / 3.86</td>
<td>8.0 M </td>
<td>The ultra-lightweight Kannada recognition model trained based on the PP-OCRv3 recognition model supports the recognition of Kannada and numbers.</td>
</tr>
<tr>
<td>ta_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/ta_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>76.83</td>
<td>5.23 / 0.75</td>
<td>10.13 / 4.30</td>
<td>8.0 M </td>
<td>The ultra-lightweight Tamil recognition model trained based on the PP-OCRv3 recognition model supports the recognition of Tamil and numbers.</td>
</tr>
<tr>
<td>latin_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/latin_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>76.93</td>
<td>5.20 / 0.79</td>
<td>8.83 / 7.15</td>
<td>7.8 M</td>
<td>The ultra-lightweight Latin recognition model trained based on the PP-OCRv3 recognition model supports the recognition of Latin script and numbers.</td>
</tr>
<tr>
<td>arabic_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/arabic_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>73.55</td>
<td>5.35 / 0.79</td>
<td>8.80 / 4.56</td>
<td>7.8 M</td>
<td>The ultra-lightweight Arabic script recognition model trained based on the PP-OCRv3 recognition model supports the recognition of Arabic script and numbers.</td>
</tr>
<tr>
<td>cyrillic_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/cyrillic_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>94.28</td>
<td>5.23 / 0.76</td>
<td>8.89 / 3.88</td>
<td>7.9 M  </td>
<td>
The ultra-lightweight cyrillic alphabet recognition model trained based on the PP-OCRv3 recognition model supports the recognition of cyrillic letters and numbers.</td>
</tr>
<tr>
<td>devanagari_PP-OCRv3_mobile_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/devanagari_PP-OCRv3_mobile_rec_infer.tar">Inference Model</a>/<a href="">Training Model</a></td>
<td>96.44</td>
<td>5.22 / 0.79</td>
<td>8.56 / 4.06</td>
<td>7.9 M  </td>
<td>The ultra-lightweight Devanagari script recognition model trained based on the PP-OCRv3 recognition model supports the recognition of Devanagari script and numbers.</td>
</tr>
</table>

<p><b>Formula Recognition Module Models</b>:</p>
<table>
<thead>
<tr>
<th>Model Name</th><th>Model Download Link</th>
<th>BLEU Score</th>
<th>Normed Edit Distance</th>
<th>ExpRate (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>LaTeX_OCR_rec</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/LaTeX_OCR_rec_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/LaTeX_OCR_rec_pretrained.pdparams">Training Model</a></td>
<td>0.8821</td>
<td>0.0823</td>
<td>40.01</td>
<td>2047.13 / 2047.13</td>
<td>10582.73 / 10582.73</td>
<td>89.7 M</td>
</tr>
</tbody>
</table>

<p><b>Seal Text Detection Module Models</b>:</p>
<table>
<thead>
<tr>
<th>Model</th><th>Model Download Link</th>
<th>Detection Hmean (%)</th>
<th>GPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>CPU Inference Time (ms)<br/>[Normal Mode / High-Performance Mode]</th>
<th>Model Size (M)</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>PP-OCRv4_server_seal_det</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-OCRv4_server_seal_det_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv4_server_seal_det_pretrained.pdparams">Training Model</a></td>
<td>98.21</td>
<td>74.75 / 67.72</td>
<td>382.55 / 382.55</td>
<td>109</td>
<td>PP-OCRv4's server-side seal text detection model, featuring higher accuracy, suitable for deployment on better-equipped servers</td>
</tr>
<tr>
<td>PP-OCRv4_mobile_seal_det</td><td><a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_inference_model/paddle3.0rc0/PP-OCRv4_mobile_seal_det_infer.tar">Inference Model</a>/<a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/official_pretrained_model/PP-OCRv4_mobile_seal_det_pretrained.pdparams">Training Model</a></td>
<td>96.47</td>
<td>7.82 / 3.09</td>
<td>48.28 / 23.97</td>
<td>4.6</td>
<td>PP-OCRv4's mobile seal text detection model, offering higher efficiency, suitable for deployment on edge devices</td>
</tr>
</tbody>
</table>

**Test Environment Description**:

- **Performance Test Environment**
  - **Test Dataset**:
    - Text Image Rectification Model: [DocUNet](https://www3.cs.stonybrook.edu/~cvl/docunet.html).
    - Layout Region Detection Model: A self-built layout analysis dataset using PaddleOCR, containing 10,000 images of common document types such as Chinese and English papers, magazines, and research reports.
    - Table Structure Recognition Model: A self-built English table recognition dataset using PaddleX.
    - Text Detection Model: A self-built Chinese dataset using PaddleOCR, covering multiple scenarios such as street scenes, web images, documents, and handwriting, with 500 images for detection.
    - Chinese Recognition Model: A self-built Chinese dataset using PaddleOCR, covering multiple scenarios such as street scenes, web images, documents, and handwriting, with 11,000 images for text recognition.
    - ch_SVTRv2_rec: Evaluation set A for "OCR End-to-End Recognition Task" in the [PaddleOCR Algorithm Model Challenge](https://aistudio.baidu.com/competition/detail/1131/0/introduction).
    - ch_RepSVTR_rec: Evaluation set B for "OCR End-to-End Recognition Task" in the [PaddleOCR Algorithm Model Challenge](https://aistudio.baidu.com/competition/detail/1131/0/introduction).
    - English Recognition Model: A self-built English dataset using PaddleX.
    - Multilingual Recognition Model: A self-built multilingual dataset using PaddleX.
    - Text Line Orientation Classification Model: A self-built dataset using PaddleX, covering various scenarios such as ID cards and documents, containing 1000 images.
    - Seal Text Detection Model: A self-built dataset using PaddleX, containing 500 images of circular seal textures.
  - **Hardware Configuration**:
    - GPU: NVIDIA Tesla T4
    - CPU: Intel Xeon Gold 6271C @ 2.60GHz
    - Other Environments: Ubuntu 20.04 / cuDNN 8.6 / TensorRT 8.5.2.2

- **Inference Mode Description**

| Mode        | GPU Configuration                        | CPU Configuration | Acceleration Technology Combination                   |
|-------------|----------------------------------------|-------------------|---------------------------------------------------|
| Normal Mode | FP32 Precision / No TRT Acceleration   | FP32 Precision / 8 Threads | PaddleInference                                 |
| High-Performance Mode | Optimal combination of pre-selected precision types and acceleration strategies | FP32 Precision / 8 Threads | Pre-selected optimal backend (Paddle/OpenVINO/TRT, etc.) |

</details>

## 2. Quick Start
The pipelines provided by PaddleX allow for quick experience of their effects. You can use the command line or Python to experience the effects of the General Layout Parsing pipeline locally.

Before using the General Layout Parsing pipeline locally, ensure you have completed the installation of the PaddleX wheel package according to the [PaddleX Local Installation Tutorial](../../../installation/installation.md).

### 2.1 Experience via Command Line
You can quickly experience the effects of the Layout Parsing pipeline with a single command. Use the [test file](https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/layout_parsing_demo.png) and replace `--input` with the local path for prediction:

```
paddlex --pipeline layout_parsing \
        --input layout_parsing_demo.png \
        --use_doc_orientation_classify False \
        --use_doc_unwarping False \
        --use_textline_orientation False \
        --save_path ./output \
        --device gpu:0
```
For parameter descriptions, refer to the parameter explanations in [2.2.2 Integration via Python Script](#222-integration-via-python-script).

After running, the results will be printed to the terminal, as shown below:

<details><summary> ğŸ‘‰Click to expand</summary>
<pre><code>{'res': {'input_path': 'layout_parsing_demo.png', 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_general_ocr': True, 'use_seal_recognition': True, 'use_table_recognition': True, 'use_formula_recognition': False}, 'parsing_res_list': [{'block_bbox': [133.37144, 40.12515, 1383.7618, 123.51433], 'block_label': 'text', 'block_content': 'åŠ©åŠ›åŒæ–¹äº¤å¾€\næ­å»ºå‹è°Šæ¡¥æ¢'}, {'block_bbox': [587.43024, 160.58405, 927.63995, 179.2846], 'block_label': 'figure_title', 'block_content': 'æœ¬æŠ¥è®°è€…æ²ˆå°æ™“ä»»å½¦é»„åŸ¹æ˜­'}, {'block_bbox': [773.798, 200.63779, 1505.5233, 687.11847], 'block_label': 'image', 'block_content': ''}, {'block_bbox': [390.42462, 201.87276, 741.41675, 292.5969], 'block_label': 'text', 'block_content': 'å„ç«‹ç‰¹é‡Œäºšé«˜ç­‰æ•™è‚²ä¸ç ”ç©¶é™¢åˆä½œå»ºç«‹ï¼Œå¼€\nè®¾äº†ä¸­å›½è¯­è¨€è¯¾ç¨‹å’Œä¸­å›½æ–‡åŒ–è¯¾ç¨‹ï¼Œæ³¨å†Œå­¦\nç”Ÿ2ä¸‡ä½™äººæ¬¡ã€‚10ä½™å¹´æ¥ï¼Œå„ç‰¹å­”é™¢å·²æˆä¸º\nå½“åœ°æ°‘ä¼—äº†è§£ä¸­å›½çš„ä¸€æ‰‡çª—å£ã€‚'}, {'block_bbox': [9.70394, 202.7036, 359.6133, 340.30905], 'block_label': 'text', 'block_content': 'èº«ç€ä¸­å›½ä¼ ç»Ÿæ°‘æ—æœè£…çš„å„ç«‹ç‰¹é‡Œäºšé’\nå¹´ä¾æ¬¡ç™»å°è¡¨æ¼”ä¸­å›½æ°‘æ—èˆã€ç°ä»£èˆã€æ‰‡å­èˆ\nç­‰ï¼Œæ›¼å¦™çš„èˆå§¿èµ¢å¾—ç°åœºè§‚ä¼—é˜µé˜µæŒå£°ã€‚è¿™\næ˜¯æ—¥å‰å„ç«‹ç‰¹é‡Œäºšé«˜ç­‰æ•™è‚²ä¸ç ”ç©¶é™¢å­”å­å­¦\né™¢(ä»¥ä¸‹ç®€ç§°"å„ç‰¹å­”é™¢"ä¸¾åŠâ€œå–œè¿æ–°å¹´"ä¸­å›½\næ­Œèˆæ¯”èµ›çš„åœºæ™¯ã€‚'}, {'block_bbox': [390.74887, 298.432, 740.7994, 436.79953], 'block_label': 'text', 'block_content': 'é»„é¸£é£è¡¨ç¤ºï¼Œéšç€æ¥å­¦ä¹ ä¸­æ–‡çš„äººæ—¥ç›Š\nå¢å¤šï¼Œé˜¿æ–¯é©¬æ‹‰å¤§å­¦æ•™å­¦ç‚¹å·²éš¾ä»¥æ»¡è¶³æ•™å­¦\néœ€è¦ã€‚2024å¹´4æœˆï¼Œç”±ä¸­ä¼èœ€é“é›†å›¢æ‰€å±å››\nå·è·¯æ¡¥æ‰¿å»ºçš„å­”é™¢æ•™å­¦æ¥¼é¡¹ç›®åœ¨é˜¿æ–¯é©¬æ‹‰å¼€\nå·¥å»ºè®¾ï¼Œé¢„è®¡ä»Šå¹´ä¸ŠåŠå¹´å³»å·¥ï¼Œå»ºæˆåå°†ä¸ºå„\nç‰¹å­”é™¢æä¾›å…¨æ–°çš„åŠå­¦åœºåœ°ã€‚'}, {'block_bbox': [10.5880165, 346.2769, 359.125, 436.1819], 'block_label': 'text', 'block_content': 'ä¸­å›½å’Œå„ç«‹ç‰¹é‡Œäºšä¼ ç»Ÿå‹è°Šæ·±åšã€‚è¿‘å¹´\næ¥,åœ¨é«˜è´¨é‡å…±å»ºâ€œä¸€å¸¦ä¸€è·¯"æ¡†æ¶ä¸‹ï¼Œä¸­å„ä¸¤\nå›½äººæ–‡äº¤æµä¸æ–­æ·±åŒ–ï¼Œäº’åˆ©åˆä½œçš„æ°‘æ„åŸºç¡€\næ—¥ç›Šæ·±åšã€‚'}, {'block_bbox': [410.5304, 457.0797, 722.77606, 516.7847], 'block_label': 'text', 'block_content': 'â€œåœ¨ä¸­å›½å­¦ä¹ çš„ç»å†\nè®©æˆ‘çœ‹åˆ°æ›´å¹¿é˜”çš„ä¸–ç•Œâ€'}, {'block_bbox': [30.340591, 457.54282, 341.95337, 516.82825], 'block_label': 'paragraph_title', 'block_content': 'â€œå­¦å¥½ä¸­æ–‡ï¼Œæˆ‘ä»¬çš„\næœªæ¥ä¸æ˜¯æ¢¦"'}, {'block_bbox': [390.90765, 538.18097, 742.19904, 604.67365], 'block_label': 'text', 'block_content': 'å¤šå¹´æ¥ï¼Œå„ç«‹ç‰¹é‡Œäºšå¹¿å¤§èµ´åç•™å­¦ç”Ÿå’Œ\nåŸ¹è®­äººå‘˜ç§¯ææŠ•èº«å›½å®¶å»ºè®¾ï¼Œæˆä¸ºåŠ©åŠ›è¯¥å›½\nå‘å±•çš„äººæ‰å’Œå„ä¸­å‹å¥½çš„è§è¯è€…å’Œæ¨åŠ¨è€…ã€‚'}, {'block_bbox': [9.953403, 538.3851, 359.45145, 652.02905], 'block_label': 'text', 'block_content': 'â€œé²œèŠ±æ›¾å‘Šè¯‰æˆ‘ä½ æ€æ ·èµ°è¿‡ï¼Œå¤§åœ°çŸ¥é“ä½ \nå¿ƒä¸­çš„æ¯ä¸€ä¸ªè§’è½â€¦â€¦"å„ç«‹ç‰¹é‡Œäºšé˜¿æ–¯é©¬æ‹‰\nå¤§å­¦ç»¼åˆæ¥¼äºŒå±‚ï¼Œä¸€é˜µä¼˜ç¾çš„æ­Œå£°åœ¨èµ°å»Šé‡Œå›\nå“ã€‚å¾ªç€ç†Ÿæ‚‰çš„æ—‹å¾‹è½»è½»æ¨å¼€ä¸€é—´æ•™å®¤çš„é—¨ï¼Œ\nå­¦ç”Ÿä»¬æ­£è·Ÿç€è€å¸ˆå­¦å”±ä¸­æ–‡æ­Œæ›²ã€ŠåŒä¸€é¦–æ­Œã€‹ã€‚'}, {'block_bbox': [390.89615, 610.6184, 741.1807, 747.9165], 'block_label': 'text', 'block_content': 'åœ¨å„ç«‹ç‰¹é‡Œäºšå…¨å›½å¦‡å¥³è”ç›Ÿå·¥ä½œçš„çº¦ç¿°\nå¨œÂ·ç‰¹éŸ¦å°”å¾·Â·å‡¯è±å¡”å°±æ˜¯å…¶ä¸­ä¸€ä½ã€‚å¥¹æ›¾åœ¨\nä¸­åå¥³å­å­¦é™¢æ”»è¯»ç¡•å£«å­¦ä½ï¼Œç ”ç©¶æ–¹å‘æ˜¯å¥³\næ€§é¢†å¯¼åŠ›ä¸ç¤¾ä¼šå‘å±•ã€‚å…¶é—´ï¼Œå¥¹å®åœ°èµ°è®¿ä¸­å›½\nå¤šä¸ªåœ°åŒºï¼Œè·å¾—äº†è§‚å¯Ÿä¸­å›½ç¤¾ä¼šå‘å±•çš„ç¬¬ä¸€\næ‰‹èµ„æ–™ã€‚'}, {'block_bbox': [10.181939, 658.8049, 359.41302, 771.31146], 'block_label': 'text', 'block_content': 'è¿™æ˜¯å„ç‰¹å­”é™¢é˜¿æ–¯é©¬æ‹‰å¤§å­¦æ•™å­¦ç‚¹çš„ä¸€\nèŠ‚ä¸­æ–‡æ­Œæ›²è¯¾ã€‚ä¸ºäº†è®©å­¦ç”Ÿä»¬æ›´å¥½åœ°ç†è§£æ­Œ\nè¯å¤§æ„ï¼Œè€å¸ˆå°¤æ–¯æ‹‰Â·ç©†ç½•é»˜å¾·è¨å°”Â·ä¾¯èµ›å› é€\nå­—ç¿»è¯‘å’Œè§£é‡Šæ­Œè¯ã€‚éšç€ä¼´å¥å£°å“èµ·ï¼Œå­¦ç”Ÿä»¬\nè¾¹å”±è¾¹éšç€èŠ‚æ‹æ‘‡åŠ¨èº«ä½“ï¼Œç°åœºæ°”æ°›çƒ­çƒˆã€‚'}, {'block_bbox': [809.68475, 705.4048, 1485.5435, 747.4364], 'block_label': 'figure_title', 'block_content': 'åœ¨å„ç«‹ç‰¹é‡Œäºšä¸ä¹…å‰ä¸¾åŠçš„ç¬¬å…­å±Šä¸­å›½é£ç­æ–‡åŒ–èŠ‚ä¸Šï¼Œå½“åœ°å°å­¦ç”Ÿä½“éªŒé£ç­åˆ¶ä½œã€‚\nä¸­å›½é©»å„ç«‹ç‰¹é‡Œäºšå¤§ä½¿é¦†ä¾›å›¾'}, {'block_bbox': [389.63492, 753.45245, 742.05634, 890.96497], 'block_label': 'text', 'block_content': 'è°ˆèµ·åœ¨ä¸­å›½æ±‚å­¦çš„ç»å†ï¼Œçº¦ç¿°å¨œè®°å¿†çŠ¹\næ–°ï¼šâ€œä¸­å›½çš„å‘å±•åœ¨å½“ä»Šä¸–ç•Œæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚\næ²¿ç€ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰é“è·¯åšå®šå‰è¡Œï¼Œä¸­å›½\nåˆ›é€ äº†å‘å±•å¥‡è¿¹ï¼Œè¿™ä¸€åˆ‡éƒ½ç¦»ä¸å¼€ä¸­å›½å…±äº§å…š\nçš„é¢†å¯¼ã€‚ä¸­å›½çš„å‘å±•ç»éªŒå€¼å¾—è®¸å¤šå›½å®¶å­¦ä¹ \nå€Ÿé‰´ã€‚â€'}, {'block_bbox': [9.884867, 777.39636, 360.3998, 843.4287], 'block_label': 'text', 'block_content': 'â€œè¿™æ˜¯ä¸­æ–‡æ­Œæ›²åˆçº§ç­ï¼Œå…±æœ‰32äººã€‚å­¦\nç”Ÿå¤§éƒ¨åˆ†æ¥è‡ªé¦–éƒ½é˜¿æ–¯é©¬æ‹‰çš„ä¸­å°å­¦ï¼Œå¹´é¾„\næœ€å°çš„ä»…æœ‰6å²ã€‚"å°¤æ–¯æ‹‰å‘Šè¯‰è®°è€…ã€‚'}, {'block_bbox': [9.801341, 850.1048, 359.61642, 1059.8444], 'block_label': 'text', 'block_content': 'å°¤æ–¯æ‹‰ä»Šå¹´23å²ï¼Œæ˜¯å„ç«‹ç‰¹é‡Œäºšä¸€æ‰€å…¬ç«‹\nå­¦æ ¡çš„è‰ºæœ¯è€å¸ˆã€‚å¥¹12å²å¼€å§‹åœ¨å„ç‰¹å­”é™¢å­¦\nä¹ ä¸­æ–‡ï¼Œåœ¨2017å¹´ç¬¬åå±Š"æ±‰è¯­æ¡¥"ä¸–ç•Œä¸­å­¦ç”Ÿ\nä¸­æ–‡æ¯”èµ›ä¸­è·å¾—å„ç«‹ç‰¹é‡Œäºšèµ›åŒºç¬¬ä¸€åï¼Œå¹¶å’Œ\nåŒä¼´ä»£è¡¨å„ç«‹ç‰¹é‡Œäºšå‰å¾€ä¸­å›½å‚åŠ å†³èµ›ï¼Œè·å¾—\nå›¢ä½“ä¼˜èƒœå¥–ã€‚2022å¹´èµ·ï¼Œå°¤æ–¯æ‹‰å¼€å§‹åœ¨å„ç‰¹å­”\né™¢å…¼èŒæ•™æˆä¸­æ–‡æ­Œæ›²ï¼Œæ¯å‘¨æœ«ä¸¤ä¸ªè¯¾æ—¶ã€‚â€œä¸­å›½\næ–‡åŒ–åšå¤§ç²¾æ·±ï¼Œæˆ‘å¸Œæœ›æˆ‘çš„å­¦ç”Ÿä»¬èƒ½å¤Ÿé€šè¿‡ä¸­\næ–‡æ­Œæ›²æ›´å¥½åœ°ç†è§£ä¸­å›½æ–‡åŒ–ã€‚"å¥¹è¯´ã€‚'}, {'block_bbox': [772.0007, 777.06, 1124.396, 1059.2354], 'block_label': 'text', 'block_content': 'â€œä¸ç®¡è¿œè¿‘éƒ½æ˜¯å®¢äººï¼Œè¯·ä¸ç”¨å®¢æ°”ï¼›ç›¸çº¦\nå¥½äº†åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬æ¬¢è¿ä½ â€¦"åœ¨ä¸€åœºä¸­å„é’\nå¹´è”è°Šæ´»åŠ¨ä¸Šï¼Œå››å·è·¯æ¡¥ä¸­æ–¹å‘˜å·¥åŒå½“åœ°å¤§\nå­¦ç”Ÿåˆå”±ã€ŠåŒ—äº¬æ¬¢è¿ä½ ã€‹ã€‚å„ç«‹ç‰¹é‡ŒäºšæŠ€æœ¯å­¦\né™¢è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹ä¸“ä¸šå­¦ç”Ÿé²å¤«å¡”Â·è°¢æ‹‰\næ˜¯å…¶ä¸­ä¸€åæ¼”å”±è€…ï¼Œå¥¹å¾ˆæ—©ä¾¿åœ¨å­”é™¢å­¦ä¹ ä¸­\næ–‡ï¼Œä¸€ç›´åœ¨ä¸ºå»ä¸­å›½ç•™å­¦ä½œå‡†å¤‡ã€‚â€œè¿™å¥æ­Œè¯\næ˜¯æˆ‘ä»¬ä¸¤å›½äººæ°‘å‹è°Šçš„ç”ŸåŠ¨å†™ç…§ã€‚æ— è®ºæ˜¯æŠ•\nèº«äºå„ç«‹ç‰¹é‡ŒäºšåŸºç¡€è®¾æ–½å»ºè®¾çš„ä¸­ä¼å‘˜å·¥ï¼Œ\nè¿˜æ˜¯åœ¨ä¸­å›½ç•™å­¦çš„å„ç«‹ç‰¹é‡Œäºšå­¦å­ï¼Œä¸¤å›½äºº\næ°‘æºæ‰‹åŠªåŠ›ï¼Œå¿…å°†æ¨åŠ¨ä¸¤å›½å…³ç³»ä¸æ–­å‘å‰å‘\nå±•ã€‚"é²å¤«å¡”è¯´ã€‚'}, {'block_bbox': [1155.9297, 777.71344, 1331.4728, 795.6411], 'block_label': 'text', 'block_content': 'ç“¦çš„åŒ—çº¢æµ·çœåšç‰©é¦†ã€‚'}, {'block_bbox': [1153.7091, 801.56256, 1504.5591, 987.63544], 'block_label': 'text', 'block_content': 'åšç‰©é¦†äºŒå±‚é™ˆåˆ—ç€ä¸€ä¸ªå‘æ˜è‡ªé˜¿æœåˆ©\næ–¯å¤åŸçš„ä¸­å›½å¤ä»£é™¶åˆ¶é…’å™¨ï¼Œç½èº«ä¸Šå†™ç€\nâ€œä¸‡â€â€œå’Œ"â€œç¦…"â€œå±±"ç­‰æ±‰å­—ã€‚â€œè¿™ä»¶æ–‡ç‰©è¯\næ˜ï¼Œå¾ˆæ—©ä»¥å‰æˆ‘ä»¬å°±é€šè¿‡æµ·ä¸Šä¸ç»¸ä¹‹è·¯è¿›è¡Œ\nè´¸æ˜“å¾€æ¥ä¸æ–‡åŒ–äº¤æµã€‚è¿™ä¹Ÿæ˜¯å„ç«‹ç‰¹é‡Œäºš\nä¸ä¸­å›½å‹å¥½äº¤å¾€å†å²çš„æœ‰åŠ›è¯æ˜ã€‚"åŒ—çº¢æµ·\nçœåšç‰©é¦†ç ”ç©¶ä¸æ–‡çŒ®éƒ¨è´Ÿè´£äººä¼Šè¨äºšæ–¯Â·ç‰¹\næ–¯æ³•å…¹å‰è¯´ã€‚'}, {'block_bbox': [390.203, 897.60095, 742.03674, 1035.7938], 'block_label': 'text', 'block_content': 'æ­£åœ¨è¥¿å—å¤§å­¦å­¦ä¹ çš„å„ç«‹ç‰¹é‡Œäºšåšå£«ç”Ÿ\nç©†å¢ç›–å¡”Â·æ³½ç©†ä¼Šå¯¹ä¸­å›½æ€€æœ‰æ·±åšæ„Ÿæƒ…ã€‚8\nç›–å¡”åœ¨ç¤¾äº¤åª’ä½“ä¸Šå†™ä¸‹è¿™æ ·ä¸€æ®µè¯ï¼šâ€œè¿™æ˜¯æˆ‘\näººç”Ÿçš„é‡è¦ä¸€æ­¥ï¼Œè‡ªæ­¤æˆ‘æ‹¥æœ‰äº†ä¸€åŒåšå›ºçš„\né‹å­ï¼Œèµ‹äºˆæˆ‘ç©¿è¶Šè†æ£˜çš„åŠ›é‡ã€‚"'}, {'block_bbox': [1154.4471, 993.4835, 1503.8441, 1107.7363], 'block_label': 'text', 'block_content': 'å„ç«‹ç‰¹é‡Œäºšå›½å®¶åšç‰©é¦†è€ƒå¤å­¦å’Œäººç±»å­¦\nç ”ç©¶å‘˜è²å°”è’™Â·ç‰¹éŸ¦å°”å¾·ååˆ†å–œçˆ±ä¸­å›½æ–‡\nåŒ–ã€‚ä»–è¡¨ç¤ºï¼šâ€œå­¦ä¹ å½¼æ­¤çš„è¯­è¨€å’Œæ–‡åŒ–ï¼Œå°†å¸®\nåŠ©å„ä¸­ä¸¤å›½äººæ°‘æ›´å¥½åœ°ç†è§£å½¼æ­¤ï¼ŒåŠ©åŠ›åŒæ–¹\näº¤å¾€ï¼Œæ­å»ºå‹è°Šæ¡¥æ¢ã€‚"'}, {'block_bbox': [391.17816, 1041.2622, 740.8725, 1131.4589], 'block_label': 'text', 'block_content': 'ç©†å¢ç›–å¡”å¯†åˆ‡å…³æ³¨ä¸­å›½åœ¨ç»æµã€ç§‘æŠ€ã€æ•™\nè‚²ç­‰é¢†åŸŸçš„å‘å±•ï¼Œâ€œä¸­å›½åœ¨ç§‘ç ”ç­‰æ–¹é¢çš„å®åŠ›\nä¸æ—¥ä¿±å¢ã€‚åœ¨ä¸­å›½å­¦ä¹ çš„ç»å†è®©æˆ‘çœ‹åˆ°æ›´å¹¿\né˜”çš„ä¸–ç•Œï¼Œä»ä¸­å—ç›ŠåŒªæµ…ã€‚â€'}, {'block_bbox': [9.486691, 1065.2955, 360.2089, 1180.0446], 'block_label': 'text', 'block_content': 'â€œå§å§ï¼Œä½ æƒ³å»ä¸­å›½å—ï¼Ÿ"â€œéå¸¸æƒ³ï¼æˆ‘æƒ³\nå»çœ‹æ•…å®«ã€çˆ¬é•¿åŸã€‚"å°¤æ–¯æ‹‰çš„å­¦ç”Ÿä¸­æœ‰ä¸€å¯¹\nèƒ½æ­Œå–„èˆçš„å§å¦¹ï¼Œå§å§éœ²å¨…ä»Šå¹´15å²ï¼Œå¦¹å¦¹\nè‰å¨…14å²ï¼Œä¸¤äººéƒ½å·²åœ¨å„ç‰¹å­”é™¢å­¦ä¹ å¤šå¹´ï¼Œ\nä¸­æ–‡è¯´å¾—æ ¼å¤–æµåˆ©ã€‚'}, {'block_bbox': [771.51514, 1065.1091, 1123.4568, 1179.5624], 'block_label': 'text', 'block_content': 'å„ç«‹ç‰¹é‡Œäºšé«˜ç­‰æ•™è‚²å§”å‘˜ä¼šä¸»ä»»åŠ©ç†è¨\né©¬ç‘è¡¨ç¤ºï¼šâ€œæ¯å¹´æˆ‘ä»¬éƒ½ä¼šç»„ç»‡å­¦ç”Ÿåˆ°ä¸­å›½è®¿\né—®å­¦ä¹ ï¼Œç›®å‰æœ‰è¶…è¿‡5000åå„ç«‹ç‰¹é‡Œäºšå­¦ç”Ÿ\nåœ¨ä¸­å›½ç•™å­¦ã€‚å­¦ä¹ ä¸­å›½çš„æ•™è‚²ç»éªŒï¼Œæœ‰åŠ©äº\næå‡å„ç«‹ç‰¹é‡Œäºšçš„æ•™è‚²æ°´å¹³ã€‚"'}, {'block_bbox': [1153.9272, 1114.0178, 1503.9585, 1347.0802], 'block_label': 'text', 'block_content': 'å„ç«‹ç‰¹é‡Œäºšå›½å®¶åšç‰©é¦†é¦†é•¿å¡”å‰ä¸Â·åŠª\né‡Œè¾¾å§†Â·ä¼˜ç´ ç¦æ›¾å¤šæ¬¡è®¿é—®ä¸­å›½ï¼Œå¯¹ä¸­åæ–‡æ˜\nçš„ä¼ æ‰¿ä¸åˆ›æ–°ã€ç°ä»£åŒ–åšç‰©é¦†çš„å»ºè®¾ä¸å‘å±•\nå°è±¡æ·±åˆ»ã€‚â€œä¸­å›½åšç‰©é¦†ä¸ä»…æœ‰è®¸å¤šä¿å­˜å®Œå¥½\nçš„æ–‡ç‰©ï¼Œè¿˜å……åˆ†è¿ç”¨å…ˆè¿›ç§‘æŠ€æ‰‹æ®µè¿›è¡Œå±•ç¤ºï¼Œ\nå¸®åŠ©äººä»¬æ›´å¥½ç†è§£ä¸­åæ–‡æ˜ã€‚"å¡”å‰ä¸è¯´ï¼Œâ€œå„\nç«‹ç‰¹é‡Œäºšä¸ä¸­å›½éƒ½æ‹¥æœ‰æ‚ ä¹…çš„æ–‡æ˜ï¼Œå§‹ç»ˆç›¸\näº’ç†è§£ã€ç›¸äº’å°Šé‡ã€‚æˆ‘å¸Œæœ›æœªæ¥ä¸ä¸­å›½åŒè¡Œ\nåŠ å¼ºåˆä½œï¼Œå…±åŒå‘ä¸–ç•Œå±•ç¤ºéæ´²å’Œäºšæ´²çš„ç¿\nçƒ‚æ–‡æ˜ã€‚â€'}, {'block_bbox': [390.8594, 1137.4973, 741.0567, 1346.7653], 'block_label': 'text', 'block_content': '23å²çš„è‰è¿ªäºšÂ·åŸƒæ–¯è’‚æ³•è¯ºæ–¯å·²åœ¨å„ç‰¹\nå­”é™¢å­¦ä¹ 3å¹´ï¼Œåœ¨ä¸­å›½ä¹¦æ³•ã€ä¸­å›½ç”»ç­‰æ–¹é¢è¡¨\nç°ååˆ†ä¼˜ç§€ï¼Œåœ¨2024å¹´å„ç«‹ç‰¹é‡Œäºšèµ›åŒºçš„\nâ€œæ±‰è¯­æ¡¥"æ¯”èµ›ä¸­è·å¾—ä¸€ç­‰å¥–ã€‚è‰è¿ªäºšè¯´ï¼šâ€œå­¦\nä¹ ä¸­å›½ä¹¦æ³•è®©æˆ‘çš„å†…å¿ƒå˜å¾—å®‰å®å’Œçº¯ç²¹ã€‚æˆ‘\nä¹Ÿå–œæ¬¢ä¸­å›½çš„æœé¥°ï¼Œå¸Œæœ›æœªæ¥èƒ½å»ä¸­å›½å­¦ä¹ ï¼Œ\næŠŠä¸­å›½ä¸åŒæ°‘æ—å…ƒç´ èå…¥æœè£…è®¾è®¡ä¸­ï¼Œåˆ›ä½œ\nå‡ºæ›´å¤šç²¾ç¾ä½œå“ï¼Œä¹ŸæŠŠå„ç‰¹æ–‡åŒ–åˆ†äº«ç»™æ›´å¤š\nçš„ä¸­å›½æœ‹å‹ã€‚â€'}, {'block_bbox': [8.70449, 1186.1178, 359.8176, 1299.481], 'block_label': 'text', 'block_content': 'éœ²å¨…å¯¹è®°è€…è¯´ï¼šâ€œè¿™äº›å¹´æ¥ï¼Œæ€€ç€å¯¹ä¸­æ–‡\nå’Œä¸­å›½æ–‡åŒ–çš„çƒ­çˆ±ï¼Œæˆ‘ä»¬å§å¦¹ä¿©å§‹ç»ˆç›¸äº’é¼“\nåŠ±ï¼Œä¸€èµ·å­¦ä¹ ã€‚æˆ‘ä»¬çš„ä¸­æ–‡ä¸€å¤©æ¯”ä¸€å¤©å¥½ï¼Œè¿˜\nå­¦ä¼šäº†ä¸­æ–‡æ­Œå’Œä¸­å›½èˆã€‚æˆ‘ä»¬ä¸€å®šè¦åˆ°ä¸­å›½\nå»ã€‚å­¦å¥½ä¸­æ–‡ï¼Œæˆ‘ä»¬çš„æœªæ¥ä¸æ˜¯æ¢¦ï¼â€'}, {'block_bbox': [9.666538, 1305.0905, 359.62704, 1347.939], 'block_label': 'text', 'block_content': 'æ®å„ç‰¹å­”é™¢ä¸­æ–¹é™¢é•¿é»„é¸£é£ä»‹ç»ï¼Œè¿™æ‰€\nå­”é™¢æˆç«‹äº2013å¹´3æœˆï¼Œç”±è´µå·è´¢ç»å¤§å­¦å’Œ'}, {'block_bbox': [791.9397, 1201.0502, 1104.4906, 1260.1833], 'block_label': 'text', 'block_content': 'â€œå…±åŒå‘ä¸–ç•Œå±•ç¤ºé\næ´²å’Œäºšæ´²çš„ç¿çƒ‚æ–‡æ˜â€'}, {'block_bbox': [772.51917, 1281.01, 1123.4009, 1348.0028], 'block_label': 'text', 'block_content': 'ä»é˜¿æ–¯é©¬æ‹‰å‡ºå‘ï¼Œæ²¿ç€èœ¿èœ“æ›²æŠ˜çš„ç›˜å±±\nå…¬è·¯ä¸€è·¯å‘ä¸œå¯»æ‰¾ä¸è·¯å°è¿¹ã€‚é©±è½¦ä¸¤ä¸ªå°\næ—¶ï¼Œè®°è€…æ¥åˆ°ä½äºå„ç«‹ç‰¹é‡Œäºšæ¸¯å£åŸå¸‚é©¬è¨'}], 'layout_det_res': {'input_path': None, 'page_index': None, 'boxes': [{'cls_id': 1, 'label': 'image', 'score': 0.9853348731994629, 'coordinate': [773.798, 200.63779, 1505.5233, 687.11847]}, {'cls_id': 2, 'label': 'text', 'score': 0.9780634045600891, 'coordinate': [772.0007, 777.06, 1124.396, 1059.2354]}, {'cls_id': 2, 'label': 'text', 'score': 0.9771724343299866, 'coordinate': [1153.9272, 1114.0178, 1503.9585, 1347.0802]}, {'cls_id': 2, 'label': 'text', 'score': 0.9763692021369934, 'coordinate': [390.74887, 298.432, 740.7994, 436.79953]}, {'cls_id': 2, 'label': 'text', 'score': 0.9752321839332581, 'coordinate': [9.70394, 202.7036, 359.6133, 340.30905]}, {'cls_id': 2, 'label': 'text', 'score': 0.9751048684120178, 'coordinate': [1153.7091, 801.56256, 1504.5591, 987.63544]}, {'cls_id': 2, 'label': 'text', 'score': 0.9741119742393494, 'coordinate': [9.801341, 850.1048, 359.61642, 1059.8444]}, {'cls_id': 2, 'label': 'text', 'score': 0.9722761511802673, 'coordinate': [390.42462, 201.87276, 741.41675, 292.5969]}, {'cls_id': 2, 'label': 'text', 'score': 0.9718317985534668, 'coordinate': [390.8594, 1137.4973, 741.0567, 1346.7653]}, {'cls_id': 2, 'label': 'text', 'score': 0.9703624844551086, 'coordinate': [390.89615, 610.6184, 741.1807, 747.9165]}, {'cls_id': 2, 'label': 'text', 'score': 0.9677473306655884, 'coordinate': [8.70449, 1186.1178, 359.8176, 1299.481]}, {'cls_id': 2, 'label': 'text', 'score': 0.9674075841903687, 'coordinate': [390.203, 897.60095, 742.03674, 1035.7938]}, {'cls_id': 2, 'label': 'text', 'score': 0.9671176075935364, 'coordinate': [389.63492, 753.45245, 742.05634, 890.96497]}, {'cls_id': 2, 'label': 'text', 'score': 0.9656032919883728, 'coordinate': [10.5880165, 346.2769, 359.125, 436.1819]}, {'cls_id': 2, 'label': 'text', 'score': 0.9655402898788452, 'coordinate': [771.51514, 1065.1091, 1123.4568, 1179.5624]}, {'cls_id': 2, 'label': 'text', 'score': 0.96494060754776, 'coordinate': [1154.4471, 993.4835, 1503.8441, 1107.7363]}, {'cls_id': 2, 'label': 'text', 'score': 0.9630844593048096, 'coordinate': [772.51917, 1281.01, 1123.4009, 1348.0028]}, {'cls_id': 2, 'label': 'text', 'score': 0.9615732431411743, 'coordinate': [9.486691, 1065.2955, 360.2089, 1180.0446]}, {'cls_id': 2, 'label': 'text', 'score': 0.9598038792610168, 'coordinate': [10.181939, 658.8049, 359.41302, 771.31146]}, {'cls_id': 2, 'label': 'text', 'score': 0.9591749310493469, 'coordinate': [391.17816, 1041.2622, 740.8725, 1131.4589]}, {'cls_id': 2, 'label': 'text', 'score': 0.9563097953796387, 'coordinate': [9.953403, 538.3851, 359.45145, 652.02905]}, {'cls_id': 2, 'label': 'text', 'score': 0.95261549949646, 'coordinate': [390.90765, 538.18097, 742.19904, 604.67365]}, {'cls_id': 2, 'label': 'text', 'score': 0.9493226408958435, 'coordinate': [9.884867, 777.39636, 360.3998, 843.4287]}, {'cls_id': 2, 'label': 'text', 'score': 0.9399433135986328, 'coordinate': [9.666538, 1305.0905, 359.62704, 1347.939]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.9254537224769592, 'coordinate': [809.68475, 705.4048, 1485.5435, 747.4364]}, {'cls_id': 2, 'label': 'text', 'score': 0.9046457409858704, 'coordinate': [1155.9297, 777.71344, 1331.4728, 795.6411]}, {'cls_id': 2, 'label': 'text', 'score': 0.8674532771110535, 'coordinate': [410.5304, 457.0797, 722.77606, 516.7847]}, {'cls_id': 0, 'label': 'paragraph_title', 'score': 0.7949447631835938, 'coordinate': [30.340591, 457.54282, 341.95337, 516.82825]}, {'cls_id': 2, 'label': 'text', 'score': 0.7313820719718933, 'coordinate': [791.9397, 1201.0502, 1104.4906, 1260.1833]}, {'cls_id': 6, 'label': 'figure_title', 'score': 0.6073322892189026, 'coordinate': [587.43024, 160.58405, 927.63995, 179.2846]}, {'cls_id': 2, 'label': 'text', 'score': 0.5846534967422485, 'coordinate': [133.37144, 40.12515, 1383.7618, 123.51433]}]}, 'overall_ocr_res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': False}, 'dt_polys': array([[[ 122,   28],
        ...,
        [ 122,  135]],

       ...,

       [[1156, 1330],
        ...,
        [1156, 1351]]], dtype=int16), 'text_det_params': {'limit_side_len': 960, 'limit_type': 'max', 'thresh': 0.3, 'box_thresh': 0.6, 'unclip_ratio': 2.0}, 'text_type': 'general', 'textline_orientation_angles': array([-1, ..., -1]), 'text_rec_score_thresh': 0, 'rec_texts': ['åŠ©åŠ›åŒæ–¹äº¤å¾€', 'æ­å»ºå‹è°Šæ¡¥æ¢', 'è¥¿', 'æœ¬æŠ¥è®°è€…æ²ˆå°æ™“ä»»å½¦é»„åŸ¹æ˜­', 'èº«ç€ä¸­å›½ä¼ ç»Ÿæ°‘æ—æœè£…çš„å„ç«‹ç‰¹é‡Œäºšé’', 'å„ç«‹ç‰¹é‡Œäºšé«˜ç­‰æ•™è‚²ä¸ç ”ç©¶é™¢åˆä½œå»ºç«‹ï¼Œå¼€', 'å¹´ä¾æ¬¡ç™»å°è¡¨æ¼”ä¸­å›½æ°‘æ—èˆã€ç°ä»£èˆã€æ‰‡å­èˆ', 'è®¾äº†ä¸­å›½è¯­è¨€è¯¾ç¨‹å’Œä¸­å›½æ–‡åŒ–è¯¾ç¨‹ï¼Œæ³¨å†Œå­¦', 'ç­‰ï¼Œæ›¼å¦™çš„èˆå§¿èµ¢å¾—ç°åœºè§‚ä¼—é˜µé˜µæŒå£°ã€‚è¿™', 'ç”Ÿ2ä¸‡ä½™äººæ¬¡ã€‚10ä½™å¹´æ¥ï¼Œå„ç‰¹å­”é™¢å·²æˆä¸º', 'æ˜¯æ—¥å‰å„ç«‹ç‰¹é‡Œäºšé«˜ç­‰æ•™è‚²ä¸ç ”ç©¶é™¢å­”å­å­¦', 'å½“åœ°æ°‘ä¼—äº†è§£ä¸­å›½çš„ä¸€æ‰‡çª—å£ã€‚', 'é™¢(ä»¥ä¸‹ç®€ç§°"å„ç‰¹å­”é™¢"ä¸¾åŠâ€œå–œè¿æ–°å¹´"ä¸­å›½', 'é»„é¸£é£è¡¨ç¤ºï¼Œéšç€æ¥å­¦ä¹ ä¸­æ–‡çš„äººæ—¥ç›Š', 'æ­Œèˆæ¯”èµ›çš„åœºæ™¯ã€‚', 'å¢å¤šï¼Œé˜¿æ–¯é©¬æ‹‰å¤§å­¦æ•™å­¦ç‚¹å·²éš¾ä»¥æ»¡è¶³æ•™å­¦', 'ä¸­å›½å’Œå„ç«‹ç‰¹é‡Œäºšä¼ ç»Ÿå‹è°Šæ·±åšã€‚è¿‘å¹´', 'éœ€è¦ã€‚2024å¹´4æœˆï¼Œç”±ä¸­ä¼èœ€é“é›†å›¢æ‰€å±å››', 'æ¥,åœ¨é«˜è´¨é‡å…±å»ºâ€œä¸€å¸¦ä¸€è·¯"æ¡†æ¶ä¸‹ï¼Œä¸­å„ä¸¤', 'å·è·¯æ¡¥æ‰¿å»ºçš„å­”é™¢æ•™å­¦æ¥¼é¡¹ç›®åœ¨é˜¿æ–¯é©¬æ‹‰å¼€', 'å›½äººæ–‡äº¤æµä¸æ–­æ·±åŒ–ï¼Œäº’åˆ©åˆä½œçš„æ°‘æ„åŸºç¡€', 'å·¥å»ºè®¾ï¼Œé¢„è®¡ä»Šå¹´ä¸ŠåŠå¹´å³»å·¥ï¼Œå»ºæˆåå°†ä¸ºå„', 'æ—¥ç›Šæ·±åšã€‚', 'ç‰¹å­”é™¢æä¾›å…¨æ–°çš„åŠå­¦åœºåœ°ã€‚', 'â€œå­¦å¥½ä¸­æ–‡ï¼Œæˆ‘ä»¬çš„', 'â€œåœ¨ä¸­å›½å­¦ä¹ çš„ç»å†', 'æœªæ¥ä¸æ˜¯æ¢¦"', 'è®©æˆ‘çœ‹åˆ°æ›´å¹¿é˜”çš„ä¸–ç•Œâ€', 'â€œé²œèŠ±æ›¾å‘Šè¯‰æˆ‘ä½ æ€æ ·èµ°è¿‡ï¼Œå¤§åœ°çŸ¥é“ä½ ', 'å¤šå¹´æ¥ï¼Œå„ç«‹ç‰¹é‡Œäºšå¹¿å¤§èµ´åç•™å­¦ç”Ÿå’Œ', 'å¿ƒä¸­çš„æ¯ä¸€ä¸ªè§’è½â€¦â€¦"å„ç«‹ç‰¹é‡Œäºšé˜¿æ–¯é©¬æ‹‰', 'åŸ¹è®­äººå‘˜ç§¯ææŠ•èº«å›½å®¶å»ºè®¾ï¼Œæˆä¸ºåŠ©åŠ›è¯¥å›½', 'å¤§å­¦ç»¼åˆæ¥¼äºŒå±‚ï¼Œä¸€é˜µä¼˜ç¾çš„æ­Œå£°åœ¨èµ°å»Šé‡Œå›', 'å‘å±•çš„äººæ‰å’Œå„ä¸­å‹å¥½çš„è§è¯è€…å’Œæ¨åŠ¨è€…ã€‚', 'å“ã€‚å¾ªç€ç†Ÿæ‚‰çš„æ—‹å¾‹è½»è½»æ¨å¼€ä¸€é—´æ•™å®¤çš„é—¨ï¼Œ', 'åœ¨å„ç«‹ç‰¹é‡Œäºšå…¨å›½å¦‡å¥³è”ç›Ÿå·¥ä½œçš„çº¦ç¿°', 'å­¦ç”Ÿä»¬æ­£è·Ÿç€è€å¸ˆå­¦å”±ä¸­æ–‡æ­Œæ›²ã€ŠåŒä¸€é¦–æ­Œã€‹ã€‚', 'å¨œÂ·ç‰¹éŸ¦å°”å¾·Â·å‡¯è±å¡”å°±æ˜¯å…¶ä¸­ä¸€ä½ã€‚å¥¹æ›¾åœ¨', 'è¿™æ˜¯å„ç‰¹å­”é™¢é˜¿æ–¯é©¬æ‹‰å¤§å­¦æ•™å­¦ç‚¹çš„ä¸€', 'ä¸­åå¥³å­å­¦é™¢æ”»è¯»ç¡•å£«å­¦ä½ï¼Œç ”ç©¶æ–¹å‘æ˜¯å¥³', 'èŠ‚ä¸­æ–‡æ­Œæ›²è¯¾ã€‚ä¸ºäº†è®©å­¦ç”Ÿä»¬æ›´å¥½åœ°ç†è§£æ­Œ', 'æ€§é¢†å¯¼åŠ›ä¸ç¤¾ä¼šå‘å±•ã€‚å…¶é—´ï¼Œå¥¹å®åœ°èµ°è®¿ä¸­å›½', 'è¯å¤§æ„ï¼Œè€å¸ˆå°¤æ–¯æ‹‰Â·ç©†ç½•é»˜å¾·è¨å°”Â·ä¾¯èµ›å› é€', 'å¤šä¸ªåœ°åŒºï¼Œè·å¾—äº†è§‚å¯Ÿä¸­å›½ç¤¾ä¼šå‘å±•çš„ç¬¬ä¸€', 'åœ¨å„ç«‹ç‰¹é‡Œäºšä¸ä¹…å‰ä¸¾åŠçš„ç¬¬å…­å±Šä¸­å›½é£ç­æ–‡åŒ–èŠ‚ä¸Šï¼Œå½“åœ°å°å­¦ç”Ÿä½“éªŒé£ç­åˆ¶ä½œã€‚', 'å­—ç¿»è¯‘å’Œè§£é‡Šæ­Œè¯ã€‚éšç€ä¼´å¥å£°å“èµ·ï¼Œå­¦ç”Ÿä»¬', 'æ‰‹èµ„æ–™ã€‚', 'ä¸­å›½é©»å„ç«‹ç‰¹é‡Œäºšå¤§ä½¿é¦†ä¾›å›¾', 'è¾¹å”±è¾¹éšç€èŠ‚æ‹æ‘‡åŠ¨èº«ä½“ï¼Œç°åœºæ°”æ°›çƒ­çƒˆã€‚', 'è°ˆèµ·åœ¨ä¸­å›½æ±‚å­¦çš„ç»å†ï¼Œçº¦ç¿°å¨œè®°å¿†çŠ¹', 'â€œè¿™æ˜¯ä¸­æ–‡æ­Œæ›²åˆçº§ç­ï¼Œå…±æœ‰32äººã€‚å­¦', 'æ–°ï¼šâ€œä¸­å›½çš„å‘å±•åœ¨å½“ä»Šä¸–ç•Œæ˜¯ç‹¬ä¸€æ— äºŒçš„ã€‚', 'â€œä¸ç®¡è¿œè¿‘éƒ½æ˜¯å®¢äººï¼Œè¯·ä¸ç”¨å®¢æ°”ï¼›ç›¸çº¦', 'ç“¦çš„åŒ—çº¢æµ·çœåšç‰©é¦†ã€‚', 'ç”Ÿå¤§éƒ¨åˆ†æ¥è‡ªé¦–éƒ½é˜¿æ–¯é©¬æ‹‰çš„ä¸­å°å­¦ï¼Œå¹´é¾„', 'æ²¿ç€ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰é“è·¯åšå®šå‰è¡Œï¼Œä¸­å›½', 'å¥½äº†åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬æ¬¢è¿ä½ â€¦â€¦â€¦"åœ¨ä¸€åœºä¸­å„é’', 'åšç‰©é¦†äºŒå±‚é™ˆåˆ—ç€ä¸€ä¸ªå‘æ˜è‡ªé˜¿æœåˆ©', 'æœ€å°çš„ä»…æœ‰6å²ã€‚"å°¤æ–¯æ‹‰å‘Šè¯‰è®°è€…ã€‚', 'åˆ›é€ äº†å‘å±•å¥‡è¿¹ï¼Œè¿™ä¸€åˆ‡éƒ½ç¦»ä¸å¼€ä¸­å›½å…±äº§å…š', 'å¹´è”è°Šæ´»åŠ¨ä¸Šï¼Œå››å·è·¯æ¡¥ä¸­æ–¹å‘˜å·¥åŒå½“åœ°å¤§', 'æ–¯å¤åŸçš„ä¸­å›½å¤ä»£é™¶åˆ¶é…’å™¨ï¼Œç½èº«ä¸Šå†™ç€', 'å°¤æ–¯æ‹‰ä»Šå¹´23å²ï¼Œæ˜¯å„ç«‹ç‰¹é‡Œäºšä¸€æ‰€å…¬ç«‹', 'çš„é¢†å¯¼ã€‚ä¸­å›½çš„å‘å±•ç»éªŒå€¼å¾—è®¸å¤šå›½å®¶å­¦ä¹ ', 'å­¦ç”Ÿåˆå”±ã€ŠåŒ—äº¬æ¬¢è¿ä½ ã€‹ã€‚å„ç«‹ç‰¹é‡ŒäºšæŠ€æœ¯å­¦', 'â€œä¸‡â€â€œå’Œâ€â€œç¦…â€â€œå±±"ç­‰æ±‰å­—ã€‚â€œè¿™ä»¶æ–‡ç‰©è¯', 'å­¦æ ¡çš„è‰ºæœ¯è€å¸ˆã€‚å¥¹12å²å¼€å§‹åœ¨å„ç‰¹å­”é™¢å­¦', 'å€Ÿé‰´ã€‚â€', 'é™¢è®¡ç®—æœºç§‘å­¦ä¸å·¥ç¨‹ä¸“ä¸šå­¦ç”Ÿé²å¤«å¡”Â·è°¢æ‹‰', 'æ˜ï¼Œå¾ˆæ—©ä»¥å‰æˆ‘ä»¬å°±é€šè¿‡æµ·ä¸Šä¸ç»¸ä¹‹è·¯è¿›è¡Œ', 'ä¹ ä¸­æ–‡ï¼Œåœ¨2017å¹´ç¬¬åå±Š"æ±‰è¯­æ¡¥"ä¸–ç•Œä¸­å­¦ç”Ÿ', 'æ­£åœ¨è¥¿å—å¤§å­¦å­¦ä¹ çš„å„ç«‹ç‰¹é‡Œäºšåšå£«ç”Ÿ', 'æ˜¯å…¶ä¸­ä¸€åæ¼”å”±è€…ï¼Œå¥¹å¾ˆæ—©ä¾¿åœ¨å­”é™¢å­¦ä¹ ä¸­', 'è´¸æ˜“å¾€æ¥ä¸æ–‡åŒ–äº¤æµã€‚è¿™ä¹Ÿæ˜¯å„ç«‹ç‰¹é‡Œäºš', 'ä¸­æ–‡æ¯”èµ›ä¸­è·å¾—å„ç«‹ç‰¹é‡Œäºšèµ›åŒºç¬¬ä¸€åï¼Œå¹¶å’Œ', 'ç©†å¢ç›–å¡”Â·æ³½ç©†ä¼Šå¯¹ä¸­å›½æ€€æœ‰æ·±åšæ„Ÿæƒ…ã€‚8', 'æ–‡ï¼Œä¸€ç›´åœ¨ä¸ºå»ä¸­å›½ç•™å­¦ä½œå‡†å¤‡ã€‚â€œè¿™å¥æ­Œè¯', 'ä¸ä¸­å›½å‹å¥½äº¤å¾€å†å²çš„æœ‰åŠ›è¯æ˜ã€‚"åŒ—çº¢æµ·', 'åŒä¼´ä»£è¡¨å„ç«‹ç‰¹é‡Œäºšå‰å¾€ä¸­å›½å‚åŠ å†³èµ›ï¼Œè·å¾—', 'æ˜¯æˆ‘ä»¬ä¸¤å›½äººæ°‘å‹è°Šçš„ç”ŸåŠ¨å†™ç…§ã€‚æ— è®ºæ˜¯æŠ•', 'çœåšç‰©é¦†ç ”ç©¶ä¸æ–‡çŒ®éƒ¨è´Ÿè´£äººä¼Šè¨äºšæ–¯Â·ç‰¹', 'å›¢ä½“ä¼˜èƒœå¥–ã€‚2022å¹´èµ·ï¼Œå°¤æ–¯æ‹‰å¼€å§‹åœ¨å„ç‰¹å­”', 'ç›–å¡”åœ¨ç¤¾äº¤åª’ä½“ä¸Šå†™ä¸‹è¿™æ ·ä¸€æ®µè¯ï¼šâ€œè¿™æ˜¯æˆ‘', 'èº«äºå„ç«‹ç‰¹é‡ŒäºšåŸºç¡€è®¾æ–½å»ºè®¾çš„ä¸­ä¼å‘˜å·¥ï¼Œ', 'æ–¯æ³•å…¹å‰è¯´ã€‚', 'é™¢å…¼èŒæ•™æˆä¸­æ–‡æ­Œæ›²ï¼Œæ¯å‘¨æœ«ä¸¤ä¸ªè¯¾æ—¶ã€‚â€œä¸­å›½', 'äººç”Ÿçš„é‡è¦ä¸€æ­¥ï¼Œè‡ªæ­¤æˆ‘æ‹¥æœ‰äº†ä¸€åŒåšå›ºçš„', 'è¿˜æ˜¯åœ¨ä¸­å›½ç•™å­¦çš„å„ç«‹ç‰¹é‡Œäºšå­¦å­ï¼Œä¸¤å›½äºº', 'å„ç«‹ç‰¹é‡Œäºšå›½å®¶åšç‰©é¦†è€ƒå¤å­¦å’Œäººç±»å­¦', 'æ–‡åŒ–åšå¤§ç²¾æ·±ï¼Œæˆ‘å¸Œæœ›æˆ‘çš„å­¦ç”Ÿä»¬èƒ½å¤Ÿé€šè¿‡ä¸­', 'é‹å­ï¼Œèµ‹äºˆæˆ‘ç©¿è¶Šè†æ£˜çš„åŠ›é‡ã€‚"', 'æ°‘æºæ‰‹åŠªåŠ›ï¼Œå¿…å°†æ¨åŠ¨ä¸¤å›½å…³ç³»ä¸æ–­å‘å‰å‘', 'ç ”ç©¶å‘˜è²å°”è’™Â·ç‰¹éŸ¦å°”å¾·ååˆ†å–œçˆ±ä¸­å›½æ–‡', 'æ–‡æ­Œæ›²æ›´å¥½åœ°ç†è§£ä¸­å›½æ–‡åŒ–ã€‚"å¥¹è¯´ã€‚', 'ç©†å¢ç›–å¡”å¯†åˆ‡å…³æ³¨ä¸­å›½åœ¨ç»æµã€ç§‘æŠ€ã€æ•™', 'å±•ã€‚"é²å¤«å¡”è¯´ã€‚', 'åŒ–ã€‚ä»–è¡¨ç¤ºï¼šâ€œå­¦ä¹ å½¼æ­¤çš„è¯­è¨€å’Œæ–‡åŒ–ï¼Œå°†å¸®', 'â€œå§å§ï¼Œä½ æƒ³å»ä¸­å›½å—ï¼Ÿ"â€œéå¸¸æƒ³ï¼æˆ‘æƒ³', 'è‚²ç­‰é¢†åŸŸçš„å‘å±•ï¼Œâ€œä¸­å›½åœ¨ç§‘ç ”ç­‰æ–¹é¢çš„å®åŠ›', 'å„ç«‹ç‰¹é‡Œäºšé«˜ç­‰æ•™è‚²å§”å‘˜ä¼šä¸»ä»»åŠ©ç†è¨', 'åŠ©å„ä¸­ä¸¤å›½äººæ°‘æ›´å¥½åœ°ç†è§£å½¼æ­¤ï¼ŒåŠ©åŠ›åŒæ–¹', 'å»çœ‹æ•…å®«ã€çˆ¬é•¿åŸã€‚"å°¤æ–¯æ‹‰çš„å­¦ç”Ÿä¸­æœ‰ä¸€å¯¹', 'ä¸æ—¥ä¿±å¢ã€‚åœ¨ä¸­å›½å­¦ä¹ çš„ç»å†è®©æˆ‘çœ‹åˆ°æ›´å¹¿', 'é©¬ç‘è¡¨ç¤ºï¼šâ€œæ¯å¹´æˆ‘ä»¬éƒ½ä¼šç»„ç»‡å­¦ç”Ÿåˆ°ä¸­å›½è®¿', 'äº¤å¾€ï¼Œæ­å»ºå‹è°Šæ¡¥æ¢ã€‚"', 'èƒ½æ­Œå–„èˆçš„å§å¦¹ï¼Œå§å§éœ²å¨…ä»Šå¹´15å²ï¼Œå¦¹å¦¹', 'é˜”çš„ä¸–ç•Œï¼Œä»ä¸­å—ç›ŠåŒªæµ…ã€‚â€', 'é—®å­¦ä¹ ï¼Œç›®å‰æœ‰è¶…è¿‡5000åå„ç«‹ç‰¹é‡Œäºšå­¦ç”Ÿ', 'å„ç«‹ç‰¹é‡Œäºšå›½å®¶åšç‰©é¦†é¦†é•¿å¡”å‰ä¸Â·åŠª', 'è‰å¨…14å²ï¼Œä¸¤äººéƒ½å·²åœ¨å„ç‰¹å­”é™¢å­¦ä¹ å¤šå¹´ï¼Œ', '23å²çš„è‰è¿ªäºšÂ·åŸƒæ–¯è’‚æ³•è¯ºæ–¯å·²åœ¨å„ç‰¹', 'åœ¨ä¸­å›½ç•™å­¦ã€‚å­¦ä¹ ä¸­å›½çš„æ•™è‚²ç»éªŒï¼Œæœ‰åŠ©äº', 'é‡Œè¾¾å§†Â·ä¼˜ç´ ç¦æ›¾å¤šæ¬¡è®¿é—®ä¸­å›½ï¼Œå¯¹ä¸­åæ–‡æ˜', 'ä¸­æ–‡è¯´å¾—æ ¼å¤–æµåˆ©ã€‚', 'å­”é™¢å­¦ä¹ 3å¹´ï¼Œåœ¨ä¸­å›½ä¹¦æ³•ã€ä¸­å›½ç”»ç­‰æ–¹é¢è¡¨', 'æå‡å„ç«‹ç‰¹é‡Œäºšçš„æ•™è‚²æ°´å¹³ã€‚"', 'çš„ä¼ æ‰¿ä¸åˆ›æ–°ã€ç°ä»£åŒ–åšç‰©é¦†çš„å»ºè®¾ä¸å‘å±•', 'éœ²å¨…å¯¹è®°è€…è¯´ï¼šâ€œè¿™äº›å¹´æ¥ï¼Œæ€€ç€å¯¹ä¸­æ–‡', 'ç°ååˆ†ä¼˜ç§€ï¼Œåœ¨2024å¹´å„ç«‹ç‰¹é‡Œäºšèµ›åŒºçš„', 'å°è±¡æ·±åˆ»ã€‚â€œä¸­å›½åšç‰©é¦†ä¸ä»…æœ‰è®¸å¤šä¿å­˜å®Œå¥½', 'â€œå…±åŒå‘ä¸–ç•Œå±•ç¤ºé', 'å’Œä¸­å›½æ–‡åŒ–çš„çƒ­çˆ±ï¼Œæˆ‘ä»¬å§å¦¹ä¿©å§‹ç»ˆç›¸äº’é¼“', 'â€œæ±‰è¯­æ¡¥"æ¯”èµ›ä¸­è·å¾—ä¸€ç­‰å¥–ã€‚è‰è¿ªäºšè¯´ï¼šâ€œå­¦', 'çš„æ–‡ç‰©ï¼Œè¿˜å……åˆ†è¿ç”¨å…ˆè¿›ç§‘æŠ€æ‰‹æ®µè¿›è¡Œå±•ç¤ºï¼Œ', 'åŠ±ï¼Œä¸€èµ·å­¦ä¹ ã€‚æˆ‘ä»¬çš„ä¸­æ–‡ä¸€å¤©æ¯”ä¸€å¤©å¥½ï¼Œè¿˜', 'ä¹ ä¸­å›½ä¹¦æ³•è®©æˆ‘çš„å†…å¿ƒå˜å¾—å®‰å®å’Œçº¯ç²¹ã€‚æˆ‘', 'æ´²å’Œäºšæ´²çš„ç¿çƒ‚æ–‡æ˜â€', 'å¸®åŠ©äººä»¬æ›´å¥½ç†è§£ä¸­åæ–‡æ˜ã€‚"å¡”å‰ä¸è¯´ï¼Œâ€œå„', 'å­¦ä¼šäº†ä¸­æ–‡æ­Œå’Œä¸­å›½èˆã€‚æˆ‘ä»¬ä¸€å®šè¦åˆ°ä¸­å›½', 'ä¹Ÿå–œæ¬¢ä¸­å›½çš„æœé¥°ï¼Œå¸Œæœ›æœªæ¥èƒ½å»ä¸­å›½å­¦ä¹ ï¼Œ', 'ç«‹ç‰¹é‡Œäºšä¸ä¸­å›½éƒ½æ‹¥æœ‰æ‚ ä¹…çš„æ–‡æ˜ï¼Œå§‹ç»ˆç›¸', 'å»ã€‚å­¦å¥½ä¸­æ–‡ï¼Œæˆ‘ä»¬çš„æœªæ¥ä¸æ˜¯æ¢¦ï¼â€', 'æŠŠä¸­å›½ä¸åŒæ°‘æ—å…ƒç´ èå…¥æœè£…è®¾è®¡ä¸­ï¼Œåˆ›ä½œ', 'ä»é˜¿æ–¯é©¬æ‹‰å‡ºå‘ï¼Œæ²¿ç€èœ¿èœ“æ›²æŠ˜çš„ç›˜å±±', 'äº’ç†è§£ã€ç›¸äº’å°Šé‡ã€‚æˆ‘å¸Œæœ›æœªæ¥ä¸ä¸­å›½åŒè¡Œ', 'æ®å„ç‰¹å­”é™¢ä¸­æ–¹é™¢é•¿é»„é¸£é£ä»‹ç»ï¼Œè¿™æ‰€', 'å‡ºæ›´å¤šç²¾ç¾ä½œå“ï¼Œä¹ŸæŠŠå„ç‰¹æ–‡åŒ–åˆ†äº«ç»™æ›´å¤š', 'å…¬è·¯ä¸€è·¯å‘ä¸œå¯»æ‰¾ä¸è·¯å°è¿¹ã€‚é©±è½¦ä¸¤ä¸ªå°', 'åŠ å¼ºåˆä½œï¼Œå…±åŒå‘ä¸–ç•Œå±•ç¤ºéæ´²å’Œäºšæ´²çš„ç¿', 'å­”é™¢æˆç«‹äº2013å¹´3æœˆï¼Œç”±è´µå·è´¢ç»å¤§å­¦å’Œ', 'çš„ä¸­å›½æœ‹å‹ã€‚â€', 'æ—¶ï¼Œè®°è€…æ¥åˆ°ä½äºå„ç«‹ç‰¹é‡Œäºšæ¸¯å£åŸå¸‚é©¬è¨', 'çƒ‚æ–‡æ˜ã€‚â€'], 'rec_scores': array([0.99982363, ..., 0.93620157]), 'rec_polys': array([[[ 122,   28],
        ...,
        [ 122,  135]],

       ...,

       [[1156, 1330],
        ...,
        [1156, 1351]]], dtype=int16), 'rec_boxes': array([[ 122, ...,  135],
       ...,
       [1156, ..., 1351]], dtype=int16)}}}
</code></pre></details>

### 2.2 Integrating via Python Script
A few lines of code suffice for rapid inference on the pipeline, taking the general layout parsing pipeline as an example:

```python
from paddlex import create_pipeline

pipeline = create_pipeline(pipeline="layout_parsing")

output = pipeline.predict(
    input="./layout_parsing_demo.png",
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False,
)
for res in output:
    res.print()  ## Print the structured output of the prediction
    res.save_to_img(save_path="./output/")  ## Save the visualized image results of all submodules for the current image
    res.save_to_json(save_path="./output/")  ## Save the structured JSON results for the current image
    res.save_to_xlsx(save_path="./output/")  ## Save the sub-table results in XLSX format for the current image
    res.save_to_html(save_path="./output/")  ## Save the sub-table results in HTML format for the current image
```

In the above Python script, the following steps are executed:

ï¼ˆ1ï¼‰Instantiate the `create_pipeline` to create a pipeline object: The specific parameter descriptions are as follows:

<table>
<thead>
<tr>
<th>Parameter</th>
<th>Parameter Description</th>
<th>Parameter Type</th>
<th>Default Value</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>pipeline</code></td>
<td>The name of the pipeline or the path to the pipeline configuration file. If it is the pipeline name, it must be a pipeline supported by PaddleX.</td>
<td><code>str</code></td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>config</code></td>
<td>The path to the pipeline configuration file.</td>
<td><code>str</code></td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td>The inference device for the pipeline. Supports specifying the specific card number for GPUs, such as "gpu:0", specific card numbers for other hardware, such as "npu:0", and CPUs as "cpu".</td>
<td><code>str</code></td>
<td><code>gpu:0</code></td>
</tr>
<tr>
<td><code>use_hpip</code></td>
<td>Whether to enable high-performance inference, only available if the pipeline supports high-performance inference.</td>
<td><code>bool</code></td>
<td><code>False</code></td>
</tr>
</tbody>
</table>

### (2) Invoke the `predict()` method of the Layout Analysis Pipeline object for inference prediction. This method will return a `generator`. Below are the parameters of the `predict()` method and their descriptions:

<table>
<thead>
<tr>
<th>Parameter</th>
<th>Parameter Description</th>
<th>Parameter Type</th>
<th>Options</th>
<th>Default Value</th>
</tr>
</thead>
<tr>
<td><code>input</code></td>
<td>The data to be predicted, supporting multiple input types, required</td>
<td><code>Python Var|str|list</code></td>
<td>
<ul>
<li><b>Python Var</b>: Such as <code>numpy.ndarray</code> representing image data</li>
<li><b>str</b>: Such as the local path of an image file or PDF file: <code>/root/data/img.jpg</code>; <b>URL link</b>, such as the network URL of an image file or PDF file: <a href="https://paddle-model-ecology.bj.bcebos.com/paddlex/imgs/demo_image/demo_paper.png">Example</a>; <b>Local directory</b>, which should contain images to be predicted, such as the local path: <code>/root/data/</code> (currently does not support prediction of PDF files within directories, PDF files need to be specified to the specific file path)</li>
<li><b>List</b>: List elements need to be of the above types of data, such as <code>[numpy.ndarray, numpy.ndarray]</code>, <code>[\"/root/data/img1.jpg\", \"/root/data/img2.jpg\"]</code>, <code>[\"/root/data1\", \"/root/data2\"]</code></li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>device</code></td>
<td>The inference device for the pipeline</td>
<td><code>str|None</code></td>
<td>
<ul>
<li><b>CPU</b>: Such as <code>cpu</code> to use CPU for inference;</li>
<li><b>GPU</b>: Such as <code>gpu:0</code> to use the first GPU for inference;</li>
<li><b>NPU</b>: Such as <code>npu:0</code> to use the first NPU for inference;</li>
<li><b>XPU</b>: Such as <code>xpu:0</code> to use the first XPU for inference;</li>
<li><b>MLU</b>: Such as <code>mlu:0</code> to use the first MLU for inference;</li>
<li><b>DCU</b>: Such as <code>dcu:0</code> to use the first DCU for inference;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline. During initialization, it will prioritize using the local GPU 0 device, and if not available, it will use the CPU device;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>use_doc_orientation_classify</code></td>
<td>Whether to use the document orientation classification module</td>
<td><code>bool|None</code></td>
<td>
<ul>
<li><b>bool</b>: <code>True</code> or <code>False</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>True</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>use_doc_unwarping</code></td>
<td>Whether to use the document distortion correction module</td>
<td><code>bool|None</code></td>
<td>
<ul>
<li><b>bool</b>: <code>True</code> or <code>False</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>True</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>use_textline_orientation</code></td>
<td>Whether to use the text line orientation classification module</td>
<td><code>bool|None</code></td>
<td>
<ul>
<li><b>bool</b>: <code>True</code> or <code>False</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>True</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>use_general_ocr</code></td>
<td>Whether to use the OCR sub-pipeline</td>
<td><code>bool|None</code></td>
<td>
<ul>
<li><b>bool</b>: <code>True</code> or <code>False</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>True</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>use_seal_recognition</code></td>
<td>Whether to use the seal recognition sub-pipeline</td>
<td><code>bool|None</code></td>
<td>
<ul>
<li><b>bool</b>: <code>True</code> or <code>False</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>True</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>use_table_recognition</code></td>
<td>Whether to use the table recognition sub-pipeline</td>
<td><code>bool|None</code></td>
<td>
<ul>
<li><b>bool</b>: <code>True</code> or <code>False</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>True</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>use_formula_recognition</code></td>
<td>Whether to use the formula recognition sub-pipeline</td>
<td><code>bool|None</code></td>
<td>
<ul>
<li><b>bool</b>: <code>True</code> or <code>False</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>True</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>layout_threshold</code></td>
<td>Score threshold for the layout model</td>
<td><code>float|dict|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number between <code>0-1</code>;</li>
<li><b>dict</b>: <code>{0:0.1}</code> where the key is the class ID and the value is the threshold for that class;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>0.5</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>layout_nms</code></td>
<td>Whether to use NMS post-processing for the layout region detection model</td>
<td><code>bool|None</code></td>
<td>
<ul>
<li><b>bool</b>: <code>True</code> or <code>False</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>True</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>layout_unclip_ratio</code></td>
<td>Expansion coefficient for the detection box of the layout region detection model</td>
<td><code>float|Tuple[float,float]|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number greater than <code>0</code>;</li>
<li><b>Tuple[float,float]</b>: Expansion coefficients in the horizontal and vertical directions respectively;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>1.0</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>layout_merge_bboxes_mode</code></td>
<td>Overlapping box filtering method for layout region detection</td>
<td><code>str|None</code></td>
<td>
<ul>
<li><b>str</b>: <code>large</code>, <code>small</code>, <code>union</code>, indicating whether to keep the larger box, smaller box, or both when overlapping boxes are filtered</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>large</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>text_det_limit_side_len</code></td>
<td>Image side length limit for text detection</td>
<td><code>int|None</code></td>
<td>
<ul>
<li><b>int</b>: Any integer greater than <code>0</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>960</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>text_det_limit_type</code></td>
<td>Image side length limit type for text detection</td>
<td><code>str|None</code></td>
<td>
<ul>
<li><b>str</b>: Supports <code>min</code> and <code>max</code>, where <code>min</code> ensures that the shortest side of the image is not less than <code>det_limit_side_len</code>, and <code>max</code> ensures that the longest side of the image is not greater than <code>limit_side_len</code></li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>max</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>text_det_thresh</code></td>
<td>Detection pixel threshold, where pixels with scores greater than this threshold in the output probability map are considered text pixels</td>
<td><code>float|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number greater than <code>0</code>
    <li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, <code>0.3</code></li></li></ul></td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>text_det_box_thresh</code></td>
<td>Detection box threshold, where detection results with an average score of all pixels within the border greater than this threshold are considered text regions</td>
<td><code>float|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number greater than <code>0</code>
    <li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, <code>0.6</code></li></li></ul></td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>text_det_unclip_ratio</code></td>
<td>Text detection expansion coefficient, which expands the text region. The larger this value, the larger the expansion area</td>
<td><code>float|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number greater than <code>0</code>
    <li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, <code>2.0</code></li></li></ul></td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>text_rec_score_thresh</code></td>
<td>Text recognition threshold, where text results with scores greater than this threshold are retained</td>
<td><code>float|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number greater than <code>0</code>
    <li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, <code>0.0</code>. I.e., no threshold is set</li></li></ul></td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>seal_det_limit_side_len</code></td>
<td>Image side length limit for seal detection</td>
<td><code>int|None</code></td>
<td>
<ul>
<li><b>int</b>: Any integer greater than <code>0</code>;</li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>960</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>seal_det_limit_type</code></td>
<td>Image side length limit type for seal detection</td>
<td><code>str|None</code></td>
<td>
<ul>
<li><b>str</b>: Supports <code>min</code> and <code>max</code>, where <code>min</code> ensures that the shortest side of the image is not less than <code>det_limit_side_len</code>, and <code>max</code> ensures that the longest side of the image is not greater than <code>limit_side_len</code></li>
<li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, initialized to <code>max</code>;</li>
</ul>
</td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>seal_det_thresh</code></td>
<td>Detection pixel threshold, where pixels with scores greater than this threshold in the output probability map are considered seal pixels</td>
<td><code>float|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number greater than <code>0</code>
    <li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, <code>0.3</code></li></li></ul></td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>seal_det_box_thresh</code></td>
<td>Detection box threshold, where detection results with an average score of all pixels within the border greater than this threshold are considered seal regions</td>
<td><code>float|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number greater than <code>0</code>
    <li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, <code>0.6</code></li></li></ul></td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>seal_det_unclip_ratio</code></td>
<td>Seal detection expansion coefficient, which expands the seal region. The larger this value, the larger the expansion area</td>
<td><code>float|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number greater than <code>0</code>
    <li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, <code>2.0</code></li></li></ul></td>
<td><code>None</code></td>
</tr>
<tr>
<td><code>seal_rec_score_thresh</code></td>
<td>Seal recognition threshold, where text results with scores greater than this threshold are retained</td>
<td><code>float|None</code></td>
<td>
<ul>
<li><b>float</b>: Any floating-point number greater than <code>0</code>
    <li><b>None</b>: If set to <code>None</code>, it will default to the value initialized by the pipeline, <code>0.0</code>. I.e., no threshold is set</li></li></ul></td>
<td><code>None</code></td>
</tr>
</table>

(3) Processing Prediction Results: Each sample's prediction result is encapsulated in a corresponding Result object, supporting operations such as printing, saving as an image, and saving as a `json` file:


<table>
<thead>
<tr>
<th>Method</th>
<th>Method Description</th>
<th>Parameters</th>
<th>Parameter Type</th>
<th>Parameter Description</th>
<th>Default Value</th>
</tr>
</thead>
<tr>
<td rowspan="3"><code>print()</code></td>
<td rowspan="3">Print results to the terminal</td>
<td><code>format_json</code></td>
<td><code>bool</code></td>
<td>Whether to format the output content with JSON indentation</td>
<td><code>True</code></td>
</tr>
<tr>
<td><code>indent</code></td>
<td><code>int</code></td>
<td>Specify the indentation level to beautify the output JSON data for better readability, only valid when <code>format_json</code> is <code>True</code></td>
<td>4</td>
</tr>
<tr>
<td><code>ensure_ascii</code></td>
<td><code>bool</code></td>
<td>Controls whether to escape non-ASCII characters to Unicode. When set to <code>True</code>, all non-ASCII characters will be escaped; <code>False</code> retains the original characters, only valid when <code>format_json</code> is <code>True</code></td>
<td><code>False</code></td>
</tr>
<tr>
<td rowspan="3"><code>save_to_json()</code></td>
<td rowspan="3">Save results as a JSON file</td>
<td><code>save_path</code></td>
<td><code>str</code></td>
<td>The file path for saving, when it is a directory, the saved file name will be consistent with the input file type</td>
<td>N/A</td>
</tr>
<tr>
<td><code>indent</code></td>
<td><code>int</code></td>
<td>Specify the indentation level to beautify the output JSON data for better readability, only valid when <code>format_json</code> is <code>True</code></td>
<td>4</td>
</tr>
<tr>
<td><code>ensure_ascii</code></td>
<td><code>bool</code></td>
<td>Controls whether to escape non-ASCII characters to Unicode. When set to <code>True</code>, all non-ASCII characters will be escaped; <code>False</code> retains the original characters, only valid when <code>format_json</code> is <code>True</code></td>
<td><code>False</code></td>
</tr>
<tr>
<td><code>save_to_img()</code></td>
<td>Save the visual images of intermediate modules in PNG format</td>
<td><code>save_path</code></td>
<td><code>str</code></td>
<td>The file path for saving, supports directories or file paths</td>
<td>N/A</td>
</tr>
<tr>
<td><code>save_to_html()</code></td>
<td>Save the tables in the file as an HTML file</td>
<td><code>save_path</code></td>
<td><code>str</code></td>
<td>The file path for saving, supports directories or file paths</td>
<td>N/A</td>
</tr>
<tr>
<td><code>save_to_xlsx()</code></td>
<td>Save the tables in the file as an XLSX file</td>
<td><code>save_path</code></td>
<td><code>str</code></td>
<td>The file path for saving, supports directories or file paths</td>
<td>N/A</td>
</tr>
</table>

- Calling the `print()` method will print the results to the terminal. The content printed to the terminal is explained as follows:
    - `input_path`: `(str)` The input path of the image to be predicted

    - `page_index`: `(Union[int, None])` If the input is a PDF file, it indicates the current page number of the PDF; otherwise, it is `None`

    - `model_settings`: `(Dict[str, bool])` Model parameters required for configuring the pipeline

        - `use_doc_preprocessor`: `(bool)` Controls whether to enable the document preprocessing pipeline
        - `use_general_ocr`: `(bool)` Controls whether to enable the OCR pipeline
        - `use_seal_recognition`: `(bool)` Controls whether to enable the seal recognition pipeline
        - `use_table_recognition`: `(bool)` Controls whether to enable the table recognition pipeline
        - `use_formula_recognition`: `(bool)` Controls whether to enable the formula recognition pipeline

    - `parsing_res_list`: `(List[Dict])` A list of parsing results, each element is a dictionary, and the list order follows the reading order after parsing.
        - `block_bbox`: `(np.ndarray)` The bounding box of the layout area.
        - `block_label`: `(str)` The label of the layout area, such as `text`, `table`, etc.
        - `block_content`: `(str)` The content within the layout area.

    - `overall_ocr_res`: `(Dict[str, Union[List[str], List[float], numpy.ndarray]])` A dictionary of global OCR results
      - `input_path`: `(Union[str, None])` The image path received by the OCR pipeline, when the input is `numpy.ndarray`, it is saved as `None`
      - `model_settings`: `(Dict)` Model configuration parameters for the OCR pipeline
      - `dt_polys`: `(List[numpy.ndarray])` A list of polygon boxes for text detection. Each detection box is represented by a numpy array consisting of 4 vertex coordinates, with a shape of (4, 2) and a data type of int16
      - `dt_scores`: `(List[float])` A list of confidence scores for text detection boxes
      - `text_det_params`: `(Dict[str, Dict[str, int, float]])` Configuration parameters for the text detection module
        - `limit_side_len`: `(int)` The side length limit for image preprocessing
        - `limit_type`: `(str)` The processing method for the side length limit
        - `thresh`: `(float)` The confidence threshold for text pixel classification
        - `box_thresh`: `(float)` The confidence threshold for text detection boxes
        - `unclip_ratio`: `(float)` The inflation coefficient for text detection boxes
        - `text_type`: `(str)` The type of text detection, currently fixed as "general"

      - `text_type`: `(str)` The type of text detection, currently fixed as "general"
      - `textline_orientation_angles`: `(List[int])` The prediction results for text line orientation classification. When enabled, it returns actual angle values (e.g., [0,0,1])
      - `text_rec_score_thresh`: `(float)` The filtering threshold for text recognition results
      - `rec_texts`: `(List[str])` A list of text recognition results, only including texts with confidence scores exceeding `text_rec_score_thresh`
      - `rec_scores`: `(List[float])` A list of confidence scores for text recognition, already filtered by `text_rec_score_thresh`
      - `rec_polys`: `(List[numpy.ndarray])` A list of text detection boxes after confidence filtering, with the same format as `dt_polys`

    - `formula_res_list`: `(List[Dict[str, Union[numpy.ndarray, List[float], str]]])` A list of formula recognition results, each element is a dictionary
        - `rec_formula`: `(str)` The formula recognition result
        - `rec_polys`: `(numpy.ndarray)` The formula detection box, with a shape of (4, 2) and a dtype of int16
        - `formula_region_id`: `(int)` The region ID where the formula is located

    - `seal_res_list`: `(List[Dict[str, Union[numpy.ndarray, List[float], str]]])` A list of seal recognition results, each element is a dictionary
        - `input_path`: `(str)` The input path of the seal image
        - `model_settings`: `(Dict)` Model configuration parameters for```markdown
**AI and Computer Vision Tutorial**

- Calling the `save_to_json()` method will save the aforementioned content to the specified `save_path`. If a directory is specified, the save path will be `save_path/{your_img_basename}.json`. If a file is specified, it will be saved directly to that file. Since JSON files do not support saving numpy arrays, `numpy.array` types will be converted to list form.
- Calling the `save_to_img()` method will save the visualization results to the specified `save_path`. If a directory is specified, the save path will be `save_path/{your_img_basename}_ocr_res_img.{your_img_extension}`. If a file is specified, it will be saved directly to that file. (pipelines often contain many result images, so it is not recommended to specify a specific file path directly, as multiple images will be overwritten, leaving only the last one.)

In addition, attributes are also supported for obtaining visual images with results and prediction results, specifically as follows:
<table>
<thead>
<tr>
<th>Attribute</th>
<th>Attribute Description</th>
</tr>
</thead>
<tr>
<td rowspan="1"><code>json</code></td>
<td rowspan="1">Obtain the predicted results in <code>json</code> format</td>
</tr>
<tr>
<td rowspan="2"><code>img</code></td>
<td rowspan="2">Obtain the visual image in <code>dict</code> format</td>
</tr>
</table>

- The prediction results obtained by the `json` attribute are data of the `dict` type, with content consistent with that saved by calling the `save_to_json()` method.
- The prediction results returned by the `img` attribute are data of the `dict` type. The keys are `layout_det_res`, `overall_ocr_res`, `text_paragraphs_ocr_res`, `formula_res_region1`, `table_cell_img`, and `seal_res_region1`, and the corresponding values are `Image.Image` objects: used to display the visual images of layout area detection, OCR, OCR text paragraphs, formulas, tables, and seal results, respectively. If optional modules are not used, only `layout_det_res` will be included in the dictionary.

Furthermore, you can obtain the layout parsing pipeline configuration file and load it for prediction. Execute the following command to save the results in `my_path`:
```
paddlex --get_pipeline_config layout_parsing --save_path ./my_path
```
Once you have obtained the configuration file, you can customize the configurations of the layout parsing pipeline by modifying the `pipeline` parameter value in the `create_pipeline` method to the path of the pipeline configuration file. An example is as follows:
```python
from paddlex import create_pipeline
pipeline = create_pipeline(pipeline="./my_path/layout_parsing.yaml")
output = pipeline.predict(
    input="./demo_paper.png",
    use_doc_orientation_classify=False,
    use_doc_unwarping=False,
    use_textline_orientation=False,
)
for res in output:
    res.print()
    res.save_to_img("./output/")
    res.save_to_json("./output/")
```
**Note**: The parameters in the configuration file are pipeline initialization parameters. If you wish to change the initialization parameters of the general layout parsing pipeline, you can directly modify the parameters in the configuration file and load it for prediction. Additionally, CLI prediction also supports passing in configuration files by specifying the path of the configuration file with `--pipeline`.

## 3. Development Integration/Deployment

If the pipeline meets your requirements in terms of inference speed and accuracy, you can proceed with development integration or deployment.

To directly apply the pipeline in your Python project, refer to the example code in [2.2 Python Script Integration](#22-python-script-integration).

Additionally, PaddleX offers three other deployment methods, detailed as follows:

ğŸš€ <b>High-Performance Inference</b>: In production environments, many applications require stringent performance metrics, especially response speed, to ensure efficient operation and smooth user experience. PaddleX provides a high-performance inference plugin that deeply optimizes model inference and pre/post-processing for significant end-to-end speedups. For detailed instructions on high-performance inference, refer to the [PaddleX High-Performance Inference Guide](../../../pipeline_deploy/high_performance_inference.en.md).

â˜ï¸ <b>Serving</b>: Serving is a common deployment strategy in real-world production environments. By encapsulating inference functions into services, clients can access these services via network requests to obtain inference results. PaddleX supports various solutions for serving pipelines. For detailed pipeline serving procedures, please refer to the [PaddleX Pipeline Serving Guide](../../../pipeline_deploy/serving.md).

Below are the API reference and multi-language service invocation examples for the basic serving solution:

<details><summary>API Reference</summary>
<p>For the main operations provided by the service:</p>
<ul>
<li>The HTTP request method is POST.</li>
<li>Both the request body and response body are JSON data (JSON objects).</li>
<li>When the request is processed successfully, the response status code is <code>200</code>, and the attributes of the response body are as follows:</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>logId</code></td>
<td><code>string</code></td>
<td>The UUID of the request.</td>
</tr>
<tr>
<td><code>errorCode</code></td>
<td><code>integer</code></td>
<td>Error code. Fixed as <code>0</code>.</td>
</tr>
<tr>
<td><code>errorMsg</code></td>
<td><code>string</code></td>
<td>Error message. Fixed as <code>"Success"</code>.</td>
</tr>
<tr>
<td><code>result</code></td>
<td><code>object</code></td>
<td>The result of the operation.</td>
</tr>
</tbody>
</table>
<ul>
<li>When the request is not processed successfully, the attributes of the response body are as follows:</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>logId</code></td>
<td><code>string</code></td>
<td>The UUID of the request.</td>
</tr>
<tr>
<td><code>errorCode</code></td>
<td><code>integer</code></td>
<td>Error code. Same as the response status code.</td>
</tr>
<tr>
<td><code>errorMsg</code></td>
<td><code>string</code></td>
<td>Error message.</td>
</tr>
</tbody>
</table>
<p>The main operations provided by the service are as follows:</p>
<ul>
<li><b><code>infer</code></b></li>
</ul>
<p>Perform layout parsing.</p>
<p><code>POST /layout-parsing</code></p>
<ul>
<li>The attributes of the request body are as follows:</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Meaning</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>file</code></td>
<td><code>string</code></td>
<td>The URL of an image or PDF file accessible by the server, or the Base64-encoded content of the above file types. For PDF files with more than 10 pages, only the content of the first 10 pages will be used.</td>
<td>Yes</td>
</tr>
<tr>
<td><code>fileType</code></td>
<td><code>integer</code>ï½œ<code>null</code></td>
<td>File type. <code>0</code> represents a PDF file, and <code>1</code> represents an image file. If this attribute is missing from the request body, the file type will be inferred based on the URL.</td>
<td>No</td>
</tr>
<tr>
<td><code>useDocOrientationClassify</code></td>
<td><code>boolean</code> | <code>null</code></td>
<td>See the description of the <code>use_doc_orientation_classify</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>useDocUnwarping</code></td>
<td><code>boolean</code> | <code>null</code></td>
<td>See the description of the <code>use_doc_unwarping</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>useTextlineOrientation</code></td>
<td><code>boolean</code> | <code>null</code></td>
<td>See the description of the <code>use_textline_orientation</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>useGeneralOcr</code></td>
<td><code>boolean</code> | <code>null</code></td>
<td>See the description of the <code>use_general_ocr</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>useSealRecognition</code></td>
<td><code>boolean</code> | <code>null</code></td>
<td>See the description of the <code>use_seal_recognition</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>useTableRecognition</code></td>
<td><code>boolean</code> | <code>null</code></td>
<td>See the description of the <code>use_table_recognition</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>useFormulaRecognition</code></td>
<td><code>boolean</code> | <code>null</code></td>
<td>See the description of the <code>use_formula_recognition</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>textDetLimitSideLen</code></td>
<td><code>integer</code> | <code>null</code></td>
<td>See the description of the <code>text_det_limit_side_len</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>textDetLimitType</code></td>
<td><code>string</code> | <code>null</code></td>
<td>See the description of the <code>text_det_limit_type</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>textDetThresh</code></td>
<td><code>number</code> | <code>null</code></td>
<td>See the description of the <code>text_det_thresh</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>textDetBoxThresh</code></td>
<td><code>number</code> | <code>null</code></td>
<td>See the description of the <code>text_det_box_thresh</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>textDetUnclipRatio</code></td>
<td><code>number</code> | <code>null</code></td>
<td>See the description of the <code>text_det_unclip_ratio</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>textRecScoreThresh</code></td>
<td><code>number</code> | <code>null</code></td>
<td>See the description of the <code>text_rec_score_thresh</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>sealDetLimitSideLen</code></td>
<td><code>integer</code> | <code>null</code></td>
<td>See the description of the <code>seal_det_limit_side_len</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>sealDetLimitType</code></td>
<td><code>string</code> | <code>null</code></td>
<td>See the description of the <code>seal_det_limit_type</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>sealDetThresh</code></td>
<td><code>number</code> | <code>null</code></td>
<td>See the description of the <code>seal_det_thresh</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>sealDetBoxThresh</code></td>
<td><code>number</code> | <code>null</code></td>
<td>See the description of the <code>seal_det_box_thresh</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>sealDetUnclipRatio</code></td>
<td><code>number</code> | <code>null</code></td>
<td>See the description of the <code>seal_det_unclip_ratio</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>sealRecScoreThresh</code></td>
<td><code>number</code> | <code>null</code></td>
<td>See the description of the <code>seal_rec_score_thresh</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>layoutThreshold</code></td>
<td><code>number</code> | <code>null</code></td>
<td>See the description of the <code>layout_threshold</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>layoutNms</code></td>
<td><code>boolean</code> | <code>null</code></td>
<td>See the description of the <code>layout_nms</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>layoutUnclipRatio</code></td>
<td><code>number</code> | <code>array</code> | <code>null</code></td>
<td>See the description of the <code>layout_unclip_ratio</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
<tr>
<td><code>layoutMergeBboxesMode</code></td>
<td><code>string</code> | <code>null</code></td>
<td>See the description of the <code>layout_merge_bboxes_mode</code> parameter in the <code>predict</code> method of the pipeline.</td>
<td>No</td>
</tr>
</tbody>
</table>
<ul>
<li>When the request is processed successfully, the response body's <code>result</code> has the following attributes:</li>
</ul>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>layoutParsingResults</code></td>
<td><code>array</code></td>
<td>The layout parsing results. The length of the array is 1 (for image input) or the smaller of the document page count and 10 (for PDF input). For PDF input, each element in the array represents the processing result of each page in the PDF file.</td>
</tr>
<tr>
<td><code>dataInfo</code></td>
<td><code>object</code></td>
<td>Information about the input data.</td>
</tr>
</tbody>
</table>
<p>Each element in <code>layoutParsingResults</code> is an <code>object</code> with the following attributes:</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Type</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>prunedResult</code></td>
<td><code>object</code></td>
<td>A simplified version of the <code>res</code> field in the JSON representation generated by the <code>predict</code> method of the production object, with the <code>input_path</code> field removed.</td>
</tr>
<tr>
<td><code>outputImages</code></td>
<td><code>object</code> | <code>null</code></td>
<td>See the description of the <code>img</code> attribute in the result of the pipeline prediction. The images are in JPEG format and encoded in Base64.</td>
</tr>
<tr>
<td><code>inputImage</code></td>
<td><code>string</code> | <code>null</code></td>
<td>The input image. The image is in JPEG format and encoded in Base64.</td>
</tr>
</tbody>
</table>
</details>

<details><summary>Multi-language Service Call Example</summary>
<details>
<summary>Python</summary>

<pre><code class="language-python">import base64
import requests

API_URL = "http://localhost:8080/layout-parsing" # Service URL
file_path = "./demo.jpg"

with open(file_path, "rb") as file:
    file_bytes = file.read()
    file_data = base64.b64encode(file_bytes).decode("ascii")

payload = {
    "file": file_data, # Base64-encoded file content or file URL
    "fileType": 1,
}

# Call the API
response = requests.post(API_URL, json=payload)

# Process the response data
assert response.status_code == 200
result = response.json()["result"]
for i, res in enumerate(result["layoutParsingResults"]):
    print(res["prunedResult"])
    for img_name, img in res["outputImages"].items():
        img_path = f"{img_name}_{i}.jpg"
        with open(img_path, "wb") as f:
            f.write(base64.b64decode(img))
        print(f"Output image saved at {img_path}")
</code></pre></details>
</details>
<br/>

ğŸ“± <b>Edge Deployment</b>: Edge deployment refers to placing computational and data processing capabilities directly on user devices, enabling them to process data without relying on remote servers. PaddleX supports deploying models on edge devices such as Android. For detailed edge deployment procedures, please refer to the [PaddleX Edge Deployment Guide](../../../pipeline_deploy/edge_deploy.en.md).

You can choose an appropriate method to deploy your model pipeline based on your needs, and proceed with subsequent AI application integration.

## 4. Custom Development
If the default model weights provided by the general layout parsing pipeline do not meet your requirements in terms of accuracy or speed for your specific scenario, you can try to further fine-tune the existing models using <b>your own domain-specific or application-specific data</b> to improve the recognition performance of the general layout parsing pipeline in your scenario.

### 4.1 Model Fine-tuning
Since the general layout parsing pipeline consists of 7 modules, unsatisfactory performance may stem from any of these modules.

You can analyze images with poor recognition results and follow the guidelines below for analysis and model fine-tuning:

* Incorrect table structure detection (e.g., wrong row/column recognition, incorrect cell positions) may indicate deficiencies in the table structure recognition module. You need to refer to the <b>Customization</b> section in the [Table Structure Recognition Module Development Tutorial](../../../module_usage/tutorials/ocr_modules/table_structure_recognition.md) and fine-tune the table structure recognition model using your private dataset.
* Misplaced layout elements (e.g., incorrect positioning of tables, seals) may suggest issues with the layout detection module. You should consult the <b>Customization</b> section in the [Layout Detection Module Development Tutorial](../../../module_usage/tutorials/ocr_modules/layout_detection.md) and fine-tune the layout detection model with your private dataset.
* Frequent undetected texts (i.e., text missing detection) indicate potential weaknesses in the text detection model. Follow the <b>Customization</b> section in the [Text Detection Module Development Tutorial](../../../module_usage/tutorials/ocr_modules/text_detection.md) to fine-tune the text detection model using your private dataset.
* High text recognition errors (i.e., recognized text content does not match the actual text) suggest further improvements to the text recognition model. Refer to the <b>Customization</b> section in the [Text Recognition Module Development Tutorial](../../../module_usage/tutorials/ocr_modules/text_recognition.md) to fine-tune the text recognition model.
* Frequent recognition errors in detected seal texts indicate the need for improvements to the seal text detection model. Consult the <b>Customization</b> section in the [Seal Text Detection Module Development Tutorials](../../../module_usage/tutorials/ocr_modules/) to fine-tune the seal text detection model.
* High recognition errors in detected formulas (i.e., recognized formula content does not match the actual formula) suggest further enhancements to the formula recognition model. Follow the [Customization](../../../module_usage/tutorials/ocr_modules/formula_recognition.md#Customization) section in the [Formula Recognition Module Development Tutorial](../../../module_usage/tutorials/ocr_modules/formula_recognition.md) to fine-tune the formula recognition model.
* Frequent misclassifications of document or certificate orientations with text areas indicate the need for improvements to the document image orientation classification model. Refer to the <b>Customization</b> section in the [Document Image Orientation Classification Module Development Tutorial](../../../module_usage/tutorials/ocr_modules/doc_img_orientation_classification.md) to fine-tune the model.

### 4.2 Model Application
After fine-tuning your model with a private dataset, you will obtain local model weights files.

To use the fine-tuned model weights, simply modify the pipeline configuration file by replacing the local paths of the fine-tuned model weights to the corresponding positions in the configuration file:

```python
......
 Pipeline:
  layout_model: PicoDet_layout_1x  # Can be modified to the local path of the fine-tuned model
  table_model: SLANet_plus  # Can be modified to the local path of the fine-tuned model
  text_det_model: PP-OCRv4_server_det  # Can be modified to the local path of the fine-tuned model
  text_rec_model: PP-OCRv4_server_rec  # Can be modified to the local path of the fine-tuned model
  formula_rec_model: LaTeX_OCR_rec  # Can be modified to the local path of the fine-tuned model
  seal_text_det_model: PP-OCRv4_server_seal_det   # Can be modified to the local path of the fine-tuned model
  doc_image_unwarp_model: UVDoc  # Can be modified to the local path of the fine-tuned model
  doc_image_ori_cls_model: PP-LCNet_x1_0_doc_ori  # Can be modified to the local path of the fine-tuned model
  layout_batch_size: 1
  text_rec_batch_size: 1
  table_batch_size: 1
  device: "gpu:0"
......
```
Subsequently, refer to the command line or Python script methods in the local experience to load the modified pipeline configuration file.

## 5. Multi-Hardware Support
PaddleX supports various mainstream hardware devices such as NVIDIA GPUs, Kunlun XPU, Ascend NPU, and Cambricon MLU. <b>Simply modify the `--device` parameter</b> to seamlessly switch between different hardware.

For example, if you use an NVIDIA GPU for inference in the layout parsing pipeline, the Python command is:

```bash
paddlex --pipeline layout_parsing --input layout_parsing.jpg --device gpu:0
```
At this point, if you want to switch the hardware to Ascend NPU, simply modify `--device` to npu in the Python command:

```bash
paddlex --pipeline layout_parsing --input layout_parsing.jpg --device npu:0
```
If you want to use the general layout parsing pipeline on more types of hardware, please refer to the [PaddleX Multi-Device Usage Guide](../../../other_devices_support/multi_devices_use_guide.md).
